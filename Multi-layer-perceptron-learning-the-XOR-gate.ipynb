{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can a vanilla multi-layer perceptron (MLP) learn the XOR gate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important point I am about to make, as it leads to alot of confusion regarded where and why the bias is introduced:\n",
    "\n",
    "With shallow models, you inject the bias term into the data matrix, giving you your [design matrix](https://en.wikipedia.org/wiki/Design_matrix). In the first notebook (linearly-separable-spaces.ipynb), we were working with a shallow model and hence introduced the bias column. \n",
    "\n",
    "When preparing data for a deep model, the biases are added **after** the input layer, and are present on the target node(s). This is just a convention. You could shift all the biases back, such that there is one present for each input node (building the design matrix), but not present on the output node.\n",
    "\n",
    "Hence the XOR gate input matrix becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 0 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "0 & 0\\\\\n",
    "\\end{bmatrix}, \n",
    "y_{xor} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\ \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input and target matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1,1],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [0,0]\n",
    "])\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the MLP.\n",
    "Notice the weights shape.\n",
    "\n",
    "_Why is it 3 rows for each?_ \n",
    "\n",
    "1 for the bias, and 2 for the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialise the weights of the network \n",
    "        3 layers:\n",
    "        hidden layer weights: (3,2) shape \n",
    "        output weights:       (3,1) shape\n",
    "        '''\n",
    "        n_in = 2\n",
    "        n_out = 1\n",
    "        Glorot_weight_init_scale =  (2/( n_in + n_out))**(1/2)\n",
    "        \n",
    "        self.W1 = np.random.normal(loc=0.0,\n",
    "                                   scale=Glorot_weight_init_scale,\n",
    "                                   size=(3,2))\n",
    "        self.W2 = np.random.normal(loc=0.0,\n",
    "                                   scale=Glorot_weight_init_scale,\n",
    "                                   size=(3,1))\n",
    "        \n",
    "    def activation(self,z):\n",
    "        '''\n",
    "        Rectified linear unit\n",
    "        '''\n",
    "        return np.maximum(z,0)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        In TF, and most frameworks, the first (or zeroth if you know you know) \n",
    "        dimension is the batch dimension \n",
    "        \n",
    "        Suppose we had a SINGLE input, x, of shape (2,). This would \n",
    "        require use to inject the addtional batch dimension \n",
    "        '''\n",
    "        if len(x.shape) == 1:\n",
    "            # testing for existence of batch dimension \n",
    "            x = x[np.newaxis,...]\n",
    "            # this adds a new axis to the front of the vector/matrix/tensor\n",
    "            # if you wanted to add to the end of it, do: \n",
    "            #  x = x[...,np.newaxis]\n",
    "            \n",
    "        b1 = np.ones(shape=(x.shape[0], 1))\n",
    "        x1_design = np.hstack(b1,x)\n",
    "        h1 = x1_design @ self.W1\n",
    "        x2 = self.activation(h1)\n",
    "        \n",
    "        b2 = np.ones(shape=(x2.shape[0], 1))\n",
    "        x2_design = np.hstack(b2,x2)\n",
    "        h2 = x2_design @ self.W2\n",
    "        \n",
    "        # linear activation\n",
    "        x3 = h2 \n",
    "        \n",
    "        return h1, x2, h2, x3 \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.13907358,  1.34213213],\n",
       "       [ 0.1300233 , -0.10749791],\n",
       "       [-0.86408775,  0.88440219]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,:][np.newaxis,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(shape=(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
