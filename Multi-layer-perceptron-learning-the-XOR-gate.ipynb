{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can a vanilla multi-layer perceptron (MLP) learn the XOR gate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important point I am about to make, as it leads to alot of confusion regarded where and why the bias is introduced:\n",
    "\n",
    "With shallow models, you inject the bias term into the data matrix, giving you your [design matrix](https://en.wikipedia.org/wiki/Design_matrix). In the first notebook (linearly-separable-spaces.ipynb), we were working with a shallow model and hence introduced the bias column. \n",
    "\n",
    "When preparing data for a deep model, the biases are added **after** the input layer, and are present on the target node(s). This is just a convention. You could shift all the biases back, such that there is one present for each input node (building the design matrix), but not present on the output node.\n",
    "\n",
    "Hence the XOR gate input matrix becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 0 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "0 & 0\\\\\n",
    "\\end{bmatrix}, \n",
    "y_{xor} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\ \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target architecture is:\n",
    "![MLP](media/MLPxorTopology.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input and target matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1,1],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [0,0]\n",
    "])\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the MLP.\n",
    "Notice the weights shape.\n",
    "\n",
    "_Why is it 3 rows for each?_ \n",
    "\n",
    "1 element for the bias, and 2 for the features! Look at the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialise the weights of the network \n",
    "        3 layers:\n",
    "        hidden layer weights: (3,2) shape \n",
    "        output weights:       (3,1) shape\n",
    "        '''\n",
    "        n_in = 2\n",
    "        n_out = 1\n",
    "        Glorot_weight_init_scale =  (2/( n_in + n_out))**(1/2)\n",
    "        \n",
    "        self.W1 = np.random.normal(loc=0.0,\n",
    "                                   scale=Glorot_weight_init_scale,\n",
    "                                   size=(3,2))\n",
    "        self.W2 = np.random.normal(loc=0.0,\n",
    "                                   scale=Glorot_weight_init_scale,\n",
    "                                   size=(3,1))\n",
    "        \n",
    "    def activation(self,z):\n",
    "        '''\n",
    "        Rectified linear unit\n",
    "        '''\n",
    "        return np.maximum(z,0)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        In TF, and most frameworks, the first (or zeroth if you know you know) \n",
    "        dimension is the batch dimension \n",
    "        \n",
    "        Suppose we had a SINGLE input, x, of shape (2,). This would \n",
    "        require use to inject the addtional batch dimension \n",
    "        '''\n",
    "        if len(x.shape) == 1:\n",
    "            # testing for existence of batch dimension \n",
    "            x = x[np.newaxis,...]\n",
    "            # this adds a new axis to the front of the vector/matrix/tensor\n",
    "            # if you wanted to add to the end of it, do: \n",
    "            #  x = x[...,np.newaxis]\n",
    "            \n",
    "        b1 = np.ones(shape=(x.shape[0], 1))\n",
    "        x1_design = np.hstack((b1,x))\n",
    "        h1 = x1_design @ self.W1\n",
    "        x2 = self.activation(h1)\n",
    "        \n",
    "        b2 = np.ones(shape=(x2.shape[0], 1))\n",
    "        x2_design = np.hstack((b2,x2))\n",
    "        h2 = x2_design @ self.W2\n",
    "        \n",
    "        # linear activation\n",
    "        x3 = h2 \n",
    "        \n",
    "        return h1, x2, h2, x3         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the MLP defined, lets setup the training routine. We need to fix some learning parameters beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100\n",
    "LR = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "MSE: 2.5205\n",
      "Epoch: 2\n",
      "MSE: 2.5205\n",
      "Epoch: 3\n",
      "MSE: 2.5205\n",
      "Epoch: 4\n",
      "MSE: 2.5205\n",
      "Epoch: 5\n",
      "MSE: 2.5205\n",
      "Epoch: 6\n",
      "MSE: 2.5205\n",
      "Epoch: 7\n",
      "MSE: 2.5205\n",
      "Epoch: 8\n",
      "MSE: 2.5205\n",
      "Epoch: 9\n",
      "MSE: 2.5205\n",
      "Epoch: 10\n",
      "MSE: 2.5205\n",
      "Epoch: 11\n",
      "MSE: 2.5205\n",
      "Epoch: 12\n",
      "MSE: 2.5205\n",
      "Epoch: 13\n",
      "MSE: 2.5205\n",
      "Epoch: 14\n",
      "MSE: 2.5205\n",
      "Epoch: 15\n",
      "MSE: 2.5205\n",
      "Epoch: 16\n",
      "MSE: 2.5205\n",
      "Epoch: 17\n",
      "MSE: 2.5205\n",
      "Epoch: 18\n",
      "MSE: 2.5205\n",
      "Epoch: 19\n",
      "MSE: 2.5205\n",
      "Epoch: 21\n",
      "MSE: 2.5205\n",
      "Epoch: 22\n",
      "MSE: 2.5205\n",
      "Epoch: 23\n",
      "MSE: 2.5205\n",
      "Epoch: 24\n",
      "MSE: 2.5205\n",
      "Epoch: 25\n",
      "MSE: 2.5205\n",
      "Epoch: 26\n",
      "MSE: 2.5205\n",
      "Epoch: 27\n",
      "MSE: 2.5205\n",
      "Epoch: 28\n",
      "MSE: 2.5205\n",
      "Epoch: 29\n",
      "MSE: 2.5205\n",
      "Epoch: 30\n",
      "MSE: 2.5205\n",
      "Epoch: 31\n",
      "MSE: 2.5205\n",
      "Epoch: 32\n",
      "MSE: 2.5205\n",
      "Epoch: 33\n",
      "MSE: 2.5205\n",
      "Epoch: 34\n",
      "MSE: 2.5205\n",
      "Epoch: 35\n",
      "MSE: 2.5205\n",
      "Epoch: 36\n",
      "MSE: 2.5205\n",
      "Epoch: 37\n",
      "MSE: 2.5205\n",
      "Epoch: 38\n",
      "MSE: 2.5205\n",
      "Epoch: 39\n",
      "MSE: 2.5205\n",
      "Epoch: 41\n",
      "MSE: 2.5205\n",
      "Epoch: 42\n",
      "MSE: 2.5205\n",
      "Epoch: 43\n",
      "MSE: 2.5205\n",
      "Epoch: 44\n",
      "MSE: 2.5205\n",
      "Epoch: 45\n",
      "MSE: 2.5205\n",
      "Epoch: 46\n",
      "MSE: 2.5205\n",
      "Epoch: 47\n",
      "MSE: 2.5205\n",
      "Epoch: 48\n",
      "MSE: 2.5205\n",
      "Epoch: 49\n",
      "MSE: 2.5205\n",
      "Epoch: 50\n",
      "MSE: 2.5205\n",
      "Epoch: 51\n",
      "MSE: 2.5205\n",
      "Epoch: 52\n",
      "MSE: 2.5205\n",
      "Epoch: 53\n",
      "MSE: 2.5205\n",
      "Epoch: 54\n",
      "MSE: 2.5205\n",
      "Epoch: 55\n",
      "MSE: 2.5205\n",
      "Epoch: 56\n",
      "MSE: 2.5205\n",
      "Epoch: 57\n",
      "MSE: 2.5205\n",
      "Epoch: 58\n",
      "MSE: 2.5205\n",
      "Epoch: 59\n",
      "MSE: 2.5205\n",
      "Epoch: 61\n",
      "MSE: 2.5205\n",
      "Epoch: 62\n",
      "MSE: 2.5205\n",
      "Epoch: 63\n",
      "MSE: 2.5205\n",
      "Epoch: 64\n",
      "MSE: 2.5205\n",
      "Epoch: 65\n",
      "MSE: 2.5205\n",
      "Epoch: 66\n",
      "MSE: 2.5205\n",
      "Epoch: 67\n",
      "MSE: 2.5205\n",
      "Epoch: 68\n",
      "MSE: 2.5205\n",
      "Epoch: 69\n",
      "MSE: 2.5205\n",
      "Epoch: 70\n",
      "MSE: 2.5205\n",
      "Epoch: 71\n",
      "MSE: 2.5205\n",
      "Epoch: 72\n",
      "MSE: 2.5205\n",
      "Epoch: 73\n",
      "MSE: 2.5205\n",
      "Epoch: 74\n",
      "MSE: 2.5205\n",
      "Epoch: 75\n",
      "MSE: 2.5205\n",
      "Epoch: 76\n",
      "MSE: 2.5205\n",
      "Epoch: 77\n",
      "MSE: 2.5205\n",
      "Epoch: 78\n",
      "MSE: 2.5205\n",
      "Epoch: 79\n",
      "MSE: 2.5205\n",
      "Epoch: 81\n",
      "MSE: 2.5205\n",
      "Epoch: 82\n",
      "MSE: 2.5205\n",
      "Epoch: 83\n",
      "MSE: 2.5205\n",
      "Epoch: 84\n",
      "MSE: 2.5205\n",
      "Epoch: 85\n",
      "MSE: 2.5205\n",
      "Epoch: 86\n",
      "MSE: 2.5205\n",
      "Epoch: 87\n",
      "MSE: 2.5205\n",
      "Epoch: 88\n",
      "MSE: 2.5205\n",
      "Epoch: 89\n",
      "MSE: 2.5205\n",
      "Epoch: 90\n",
      "MSE: 2.5205\n",
      "Epoch: 91\n",
      "MSE: 2.5205\n",
      "Epoch: 92\n",
      "MSE: 2.5205\n",
      "Epoch: 93\n",
      "MSE: 2.5205\n",
      "Epoch: 94\n",
      "MSE: 2.5205\n",
      "Epoch: 95\n",
      "MSE: 2.5205\n",
      "Epoch: 96\n",
      "MSE: 2.5205\n",
      "Epoch: 97\n",
      "MSE: 2.5205\n",
      "Epoch: 98\n",
      "MSE: 2.5205\n",
      "Epoch: 99\n",
      "MSE: 2.5205\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "\n",
    "loss_hist, W1_hist, W2_hist = [],[],[]\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # forward pass\n",
    "    h1, x2, h2, x3 = model.forward(X)\n",
    "    \n",
    "    loss = (1/4) * np.sum((x3 -Y)**2)/len(Y)\n",
    "    \n",
    "    loss_hist.append(loss)\n",
    "    \n",
    "    if epoch % 20:\n",
    "        print(f\"Epoch: {epoch}\\nMSE: {loss:.4f}\")\n",
    "    \n",
    "    # This is backpropagation. Note that many frameworks \n",
    "    # have implemented automatic differentiation techniques \n",
    "    # so that you dont need to concern yourself with \n",
    "    # the derivatives of activations and weight updates. \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_to_optimal_weights(model):\n",
    "    '''\n",
    "    Sets weights of MLP to optimal set for XOR gate modelling.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model.W1 = np.array([[0, -1],\n",
    "                         [1,1],\n",
    "                         [1,1]], dtype=np.float)\n",
    "    model.W2 = np.array([[0],\n",
    "                         [1],\n",
    "                         [-2]], dtype=np.float)\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
