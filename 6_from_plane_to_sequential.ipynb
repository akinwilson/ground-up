{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "import sympy as sy\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},\\theta)  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta \\text{: parameters shared across all time steps}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, its state at time step t, is dependent only on a set a parameters and the previous state at t-1\n",
    "<br>\n",
    "<br>\n",
    "Let the state of the system, h, also be depedent on an input at the respective time step, x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},x^{t},\\theta)  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state h now contains information about the entire past history of inputs, x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a system that given the hidden state, h,produces an output o, for each time step. This output is passed to an activation function made to predict the target, y, at the respective time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "o^t= g (h^{t},\\theta')  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta' \\text{: a different set of parameters as $\\theta$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now define $\\theta$ and $\\theta'$ as the weight matrices describing the relation between the input-to-hidden, hidden-to-hidden and hidden-to-output notes; $U$, $W$ and $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z^t=  W^{T}h^{t-1} + U^{T}x^t +b \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{t} = \\phi(z^t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o^t = V^Th^{t} + c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $b$ and $c$ are biases, $\\phi$ is an activation function. <br><br>\n",
    "**Note**: matrices $U$, $W$ and $V$ are not indexed by time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following schematic to get a better understanding for a reccurent system\n",
    "<img src=\"media/RNNFoldedandUnfolded.png\" style=\"height: 300px;\"/>\n",
    "credits: fdeloche "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each time step, we have a sequential total loss up to time step $\\tau$, $L^\\tau$, defined as the difference between our prediction and the target, at each output, upto the time step $\\tau$\n",
    "<br>\n",
    "<br>\n",
    "Consider the task of multi-class classification. \n",
    "<br>\n",
    "<br>\n",
    "Consequently, the output activation function is the normalized expontential function, a.k.a the _softmax function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\sum_{t=1}^{\\tau} l\\big(o^{t}\\big)\n",
    "\\text{: Total loss upto time step $\\tau$}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\text{: Softmax activation function for multi-class classification}\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE** the softmax is a vector function, later when taking the derivative, in reality I am finding the Jacobian of it in its vector form, but here I denote one element of it, the $i^{th}$\n",
    "\n",
    "\\begin{equation}\n",
    "l = - \\sum_{m=0}^{M-1}y_{m}^{t} \\log\\Big(\\hat{y}_{m}^{t}\\Big)\n",
    "\\text{: M categorical cross entropy for predictions at time step $t$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process differs from standard back-propagation (like descirbed for a vanilla feedforward network). Usng the above assumptions, I will go through the derivation analogous optimization process for recurrent networks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation through time\n",
    "\n",
    "Per example loss w.r.t to the output element $o_i$ at time $t$; $o_i^{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{o_{i}^{t}} L = \\frac{\\partial{L}}{\\partial{l(o_i^t)}} \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "Note that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{L}}{\\partial{l(o_i^t)}} = 1\n",
    "\\end{equation}\n",
    "and that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "is the derivative of the categorical cross-entropy\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}}\\frac{\\partial{\\hat{y}^{t}_j}}{\\partial{o_i^{t}}} } - [1]\n",
    "\\end{equation}\n",
    "The softmax functions is:\n",
    " \\begin{equation}\n",
    " \\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\end{equation}\n",
    "Taking its derivative gives:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "    \\frac{\\partial{\\hat{y}^{t}_i}}{\\partial{o_j^{t}}} = \\hat{y}^{t}_{i} \\Big( \\delta_{ij}  -  \\hat{y}^{t}_{j} \\Big)\n",
    "}- [2]\n",
    "\\end{equation}\n",
    "_look at the different cases to see why this is true_ i.e. $i=j$ and $i \\neq j$\n",
    "<br><br>\n",
    "Lets sub [2] into [1], and splitting into the cases where $i=j$ and $i \\neq j $:\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}} \\hat{y}^{t}_{j} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    " \n",
    "Lets now split the sum up for the two cases;\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j=i} + \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j \\neq i}  =  -y^{t}_{i}(\\delta_{ii} - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "Simplfying down: \n",
    "\n",
    "\\begin{equation}\n",
    " \n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big( 0 -\\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} \n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}+y^{t}_{i}\\hat{y}_{i}^{t}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "Recall that $\\sum_{j} y_j = 1$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} \\Big( y_j^{t} +y^{t}_{i} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j} \\Big( y_j^{t} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets calculate the gradient on the internel nodes $h^t$ from the end of the sequence $\\tau$.\n",
    "<br>\n",
    "I am going to use vector notation here on out. I.e. $h_i^{t}$ becomes $h^t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = \\Bigg( \\frac{ \\partial{o^{\\tau}}}\n",
    "{\\partial{h^{\\tau}}} \\Bigg)^{T} \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = V \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "we iterate backwards through time. Note the dependency of $h^t$ on both $o^t$ and $h^{t+1}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = \\Bigg( \\frac{ \\partial{h^{t+1}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{h^{t+1}} L +\n",
    "\\Bigg( \\frac{ \\partial{o^{t}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{o^{t}} L \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate of the hidden units  w.r.t their previous time step is:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  \\frac{ \\partial{h^{t+1}} }{ \\partial{z^{t+1} } }\n",
    "\\frac{ \\partial{z^{t+1} } } { \\partial{h^{t}} }\n",
    "\\end{equation}\n",
    "This leads to:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  diag\\Bigg( \\phi'\\big(z^{t+1}\\big) \\Bigg) W^T\n",
    "\\end{equation}\n",
    "**Note** diag: considering only the leading diagonal values and setting all others to 0. \n",
    "<br><br>\n",
    "For RNNs , we want to use a saturating activation to avoid gradient explosions <br><br>\n",
    "e.g. hyperbolic tagent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \\phi'\\big(z^{t+1}\\big) \\Big)   \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets specify the activation function (using the hyperpolic tagent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \n",
    "     1 - \\big(h^{t+1}\\big)^2\n",
    "    \\Big)  \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradients on the biases $b$ and $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\Bigg(\n",
    "     \\frac{\\partial{o^t}}{\\partial{c^t}} \n",
    "     \\Bigg)^{T} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "since $\\frac{\\partial{o^t}}{\\partial{c^t}} = 1$\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  \\Bigg(\n",
    "     \\frac{\\partial{h^t}}{\\partial{b^t}} \n",
    "     \\Bigg)^{T}  \\nabla_{h^t} L\n",
    "\\end{equation}\n",
    "Since $b$ is dependent on h through the activation function $\\phi$, we have: \n",
    "\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  diag \\Bigg( \\phi' \\Big( z^t \\Big) \\Bigg) \\nabla_{h^t} L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative w.r.t to $V$; the hidden-ouput matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{V} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial o_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{V} O_i^{t}\n",
    "\\end{equation}\n",
    "Leading to:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{V} L  = \\sum_{t} h^t \\Big(\\nabla_{o^t} L \\Big)^T\n",
    "}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative w.r.t the weight matrices $W$ and $U$, we introduce dummy variables $W^t$ and $U^t$. These are copies of each other at each time step, summing these up will give us the total gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{W} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{W^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{W} L  = \\sum_{t} h^{t-1} \\Big(\\nabla_{h^t} L \\Big)^T  diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n",
    "for the derivative of w.r.t $U$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{U} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{U^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{U} L  = \\sum_{t} x^{t} \\Big( \\nabla_{h^t} L \\Big)^T \n",
    "     diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network implementation with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        # network variables \n",
    "        self.idim = input_dim\n",
    "        self.hdim = hidden_dim\n",
    "        self.odim = output_dim\n",
    "        # initialise weights \n",
    "        self.U = np.random.uniform(- np.sqrt(1./self.idim),\n",
    "                                     np.sqrt(1./self.idim),\n",
    "                                    (self.idim, self.hdim) )\n",
    "\n",
    "        self.V = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.odim))\n",
    "\n",
    "        self.W = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.hdim))\n",
    "\n",
    "        self.b = np.zeros(self.hdim)\n",
    "        self.c = np.zeros(self.odim)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        Note that this is a numerically stable version of softmax.\n",
    "        We substract the max value from all elements.\n",
    "        Overflow of a single element, or underflow of all elements,  will render the output usless.\n",
    "        subtracting max leaves only non-positive values ---> no overflow \n",
    "        at least one element = 0 ---> no vanishing denominator (underflow is some enteries is okay) \n",
    "         '''\n",
    "        xt = np.exp(x-np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Single example pass forward, all the way through the network\n",
    "        T = len(x)\n",
    "        # will stack as rows\n",
    "        h = np.zeros((T,self.hdim))\n",
    "        o = np.zeros((T,self.odim))\n",
    "        for t in range(T):\n",
    "            h[t] = self.U.T @ x[t] + self.b\n",
    "            if t > 1:\n",
    "                h[t] += self.W @ h[t-1] + self.b\n",
    "            h[t] = np.tanh(h[t])\n",
    "            o[t] = self.softmax( self.V.T @ h[t] + self.c)\n",
    "        return (o,h)\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, x, y, clip=None):\n",
    "        T = len(x)\n",
    "        o,h = self.forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdb = np.zeros(self.b.shape)\n",
    "        dLdc = np.zeros(self.c.shape)\n",
    "        # dL/do\n",
    "        delta_o = o\n",
    "        # Notice, only evaluting at last output of network, yHat - y \n",
    "        delta_o[ np.arange(T), y ] -= float(y) \n",
    "        # dL/dh\n",
    "        delta_h = np.zeros((T, self.hdim))\n",
    "        for t in reversed(range(T)):\n",
    "\n",
    "            # collect errors on hidden states\n",
    "            delta_h[t] = self.V @ delta_o[T-1,:]\n",
    "            if t < T-1:\n",
    "                # collect errors on hidden states due to W\n",
    "                delta_h[t] = ( self.W @ np.diag(1-h[t+1]**2) ) @ delta_h[t+1]\n",
    "        for t in range(T):\n",
    "            # error on ouput bias\n",
    "            dLdc += delta_o[T-1,:]\n",
    "            # error on hidden bias \n",
    "            dLdb += (1-h[t]**2) * delta_h[t,:]\n",
    "            # error on hidden-output matrix\n",
    "            ot = delta_o[T-1,:][...,np.newaxis]\n",
    "            ht = h[t,:][...,np.newaxis]\n",
    "            dht = delta_h[t,:][...,np.newaxis]\n",
    "\n",
    "            dLdV += ht @ ot.T \n",
    "            # error on hidden-hidden W\n",
    "            if t > 0 :\n",
    "                h_t = h[t-1,:][...,np.newaxis]\n",
    "                dLdW += ( h_t @ dht.T )@np.diag(1-h[t]**2)\n",
    "            xt = x[t][...,np.newaxis]\n",
    "            dLdU += xt @ dht.T @ np.diag(1-h[t]**2)\n",
    "\n",
    "        if clip is not None:\n",
    "            dLdb = np.clip(dLdb, -clip, clip)\n",
    "            dLdc = np.clip(dLdc, -clip, clip)\n",
    "            dLdV = np.clip(dLdV, -clip, clip)\n",
    "            dLdW = np.clip(dLdW, -clip, clip)\n",
    "            dLdU = np.clip(dLdU, -clip, clip)\n",
    "        return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
    "\n",
    "\n",
    "    def step(self,x,y,lr=0.0001):\n",
    "        dLdU, dLdV, dLdW, dLdb, dLdc = self.backward(x,y)\n",
    "        self.U -= lr * dLdU\n",
    "        self.V -= lr * dLdV\n",
    "        self.W -= lr * dLdW \n",
    "        self.b -= lr * dLdb \n",
    "        self.c -= lr * dLdc \n",
    "    \n",
    "\n",
    "    def Loss(self, x,y):\n",
    "        o,h = self.forward(x)      \n",
    "        yHat= o[len(x)-1, :]\n",
    "        y_1h = [0.0]*len(yHat)\n",
    "        y_1h[int(y)] = 1.0\n",
    "        LOSS = self.categorical_cross_entropy_loss(yHat, y_1h)\n",
    "        return LOSS\n",
    "\n",
    "\n",
    "    def categorical_cross_entropy_loss(self, yHats, ys):\n",
    "        loss = 0.0\n",
    "        for pred,true in zip(yHats, ys):\n",
    "            loss += -1.0*(true * np.log(pred) + (1.0-true) * np.log(1.0 - pred))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling synthetic time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZMklEQVR4nO3de7QdZXnH8e+PJKBVKpEcFXIli2jFKipHiKWlUNACpUStVsAL2tK0LqDY0mUptNray6Ja76a1WSEVNOIFRVOIclEEXRrkHMotidRjSkoilGOIiNdwzNM/9kQ3J3ufy545M7Pn/X3WOit7Lmfed3bmPPPOM+87o4jAzMyab7+qK2BmZuVwwDczS4QDvplZIhzwzcwS4YBvZpYIB3wzs0Q44Fvfk/R5SWdXXQ+zupP74VvVJN0H/BJwWET8MJt3DvDaiDi+wqqZNYpb+FYXs4ALqq6EWZM54FtdvBP4C0kHdVoo6dck3SbpkezfX2tb9uXsigBJh0u6OVvvu5I+kc1fJeld47a5XtKfdSnvOZJukPSwpP+TdHE2/wBJ75X0neznvZIOyJbNk3SNpO9lv/cVSfv8jUn6O0kfyD7PkfRDSe/Mpp8o6SeSnppNf0rSg9n+3CLpOdn8Y7L5s9q2+3JJd2Wf95N0kaRvS9op6ZN7t2npcsC3uhgCvgz8xfgFWaC6Fng/cDDwbuBaSQd32M7fA9cDc4EFwAey+ZcDZ+4NwJLmAScBH+tQ3oHAjcAXgEOBw4EvZosvAZYDzweOBI4G/jpbdiGwHRgAng5cDHTKmd4MHJ99fhHwIHBcNv1i4N6IeDib/jywDHgacDuwDiAibgV+CPxW23bPatuf84GXAb+Z7cMuYFWHulhCHPCtTt4KnC9pYNz83wG+FREfiYixiLgS+Cbwux228RiwGDg0In4SEV8FiIhvAI8AJ2brnQF8OSL+r8M2TgMejIh3Zdt4NAuwAK8B3h4RD0XEKPB3wOvayj4EWBwRj0XEV6LzTbKvA8uyE9ZxwGXAfElPphWgb967YkSszcr/KfC3wJGSnpItvhI4E35+kjo1mwfwJ8AlEbG97XdfKWl2h/pYIhzwrTYi4h7gGuCicYsOBbaNm7cNmN9hM28BBHxD0iZJf9C27HLgtdnn1wIf6VKVhcC3uywbX5dt2TxopaVGgOslbZU0fj8AiIgf07qi+U1aAf9m4GvAsbQFfEmzJF2apWW+D9yXbWJe9u/HgFdkKaVXALdHxN66LQauztJL3wO2AD+jdeVhiXLAt7p5G/BHPD6Yf4dWAGu3CNgx/pcj4sGI+KOIOBT4Y+BfJR2eLf4osELSkcCzgc92qcP9wNIuy8bXZVE2j6wlfmFELAVOB/5c0okdtgGtoP5bwAuA27Lp36aVIrolW+csYAWt1NNTgCXZfGXlbaZ1wjmFx6dz9u7DKRFxUNvPEyJin+/M0uGAb7USESPAJ4A/bZu9AXimpLMkzZb0auAIWlcDjyPpVZIWZJO7aOXQ92Tb3k4ruH4E+HTW0u7kGuAQSW/ObtIeKOmYbNmVwF9LGsjuA7yV1okESadlN41FK330s71ld3Az8Hpgc0TspnX/4hzgf7JUEcCBwE+BnbS6rf5Th+18jFbvpuOAT7XN/xDwj5IWZ3UbkLSiS10sEQ74VkdvB560dyIidtLKq19IK/i9BTgtIr7b4XdfBNwq6QfAeuCCiNjatvxy4Ll0T+cQEY8CL6F1j+BB4FvACdnif6CVjrkLuJvWjdR/yJYto3Wz9we08vT/GhE3dSnma8AT+UVrfjPwk7ZpgCtoteB3ZMs3dtjOlbTSQF8a9328j9b+Xy/p0ex3j+nw+5YQD7yypEg6jlaLfHGXG6pmjeUWviVD0hxa6Y81DvaWIgd8S4KkZwPfo9Vt8r2VVsasIk7pmJklwi18M7NE1HrU3bx582LJkiVVV8PMrG8MDw9/NyLGj1YHah7wlyxZwtDQUNXVMDPrG5LGj0r/Oad0zMwS4YBvZpYIB3wzs0Q44JuZJcIB38wsEQ74ZmaJcMA3M6uR4W27WHXTCMPbdhW+7Vr3wzczS8nwtl28Zs1Gdo/tYf/Z+7HunOUctXhuYdt3C9/MrCY2bt3J7rE97Al4bGwPG7fuLHT7DvhmZjWxfOnB7D97P2YJ5szej+VLDy50+07pmJnVxFGL57LunOVs3LqT5UsPLjSdAw74Zma1ctTiuYUH+r2c0jEzS4QDvplZIhzwzcwSkTvgS1oo6SZJmyVtknRBh3Uk6f2SRiTdJemFecs1M7PpKeKm7RhwYUTcLulAYFjSDRGxuW2dU4Bl2c8xwL9l/5qZWUlyt/Aj4oGIuD37/CiwBZg/brUVwBXRshE4SNIhecs2M7OpKzSHL2kJ8ALg1nGL5gP3t01vZ9+Twt5trJQ0JGlodHS0yOqZmSWtsIAv6cnAp4E3R8T3e91ORKyOiMGIGBwY6PgeXjMz60EhAV/SHFrBfl1EfKbDKjuAhW3TC7J5ZmZWkiJ66Qi4DNgSEe/ustp64PVZb53lwCMR8UDess3MbOqK6KVzLPA64G5Jd2TzLgYWAUTEh4ANwKnACPAj4I0FlGtmZtOQO+BHxFcBTbJOAOfmLcvMzHrnkbZmZolwwDczS4QDvplZIhzwzcwS4YBvZpYIB3wzs0Q44JuZJcIB38wsEQ74ZmaJcMA3M0uEA76ZWSIc8M3MEuGAb2aWCAd8M7NEOOCbmSXCAd/MLBFFvdN2raSHJN3TZfnxkh6RdEf289YiyjUzs6kr4hWHAB8GPghcMcE6X4mI0woqz8zMpqmQFn5E3AI8XMS2zMxsZpSZw3+xpDslfV7Sc7qtJGmlpCFJQ6OjoyVWz8ys2coK+LcDiyPiSOADwGe7rRgRqyNiMCIGBwYGSqqemVnzlRLwI+L7EfGD7PMGYI6keWWUbWZmLaUEfEnPkKTs89FZuTvLKNvMzFoK6aUj6UrgeGCepO3A24A5ABHxIeCVwJskjQE/Bs6IiCiibDMzm5pCAn5EnDnJ8g/S6rZpZmYV8UhbM7NEOOCbmSWikQF/eNsuVt00wvC2XVVXxcysNop6tEJtDG/bxWvWbGT32B72n70f685ZzlGL51ZdLTOzyjWuhb9x6052j+1hT8BjY3vYuNW9P83qwFfe1WtcC3/50oPZf/Z+PDa2hzmz92P50oOrrpJZ8nzlXQ+NC/hHLZ7LunOWs3HrTpYvPdgHlVkNdLry9t9m+RoX8KEV9FM8mIa37fKJzmrJV9710MiAnyJfMlsddGt0+Mq7HhzwG8KXzFa1yRodqV5510njeumkau8l8yzhS2arhHvI1Z9b+A3hS2armvP09ac6P7RycHAwhoaGqq6GmU2ROw5UT9JwRAx2WuYWvpkVxnn6enMO38wsEQ74ZmaJcMA3M0tEIQFf0lpJD0m6p8tySXq/pBFJd0l6YRHlmpn1qyoeJlfUTdsP03qF4RVdlp8CLMt+jgH+LfvXzCw5VY2ML6SFHxG3AA9PsMoK4Ipo2QgcJOmQIsq2yfmxtFY0H1P5VDVIraxumfOB+9umt2fzHhi/oqSVwEqARYsWlVK5JvMzdqxoPqbyq2qQWu1u2kbE6ogYjIjBgYGBqqvT9zzc3YrmYyq/vSPj//ylzyr1hFlWC38HsLBtekE2z2aYh7tb0co6ppoyaneiJ4iWvV9lBfz1wHmSPk7rZu0jEbFPOseK52fsFKMpwacIZRxTTUkb1W0/Cgn4kq4EjgfmSdoOvA2YAxARHwI2AKcCI8CPgDcWUa5NjYe751O3P9o6mOljqimP+67bfhQS8CPizEmWB3BuEWWZla1uf7QpaEoqsm774YenmU2ibn+0KWhKKrJu++HHI/ch55PL5+/c+oUfj9wgzidXw/dBrAlq1w9/JjVhdKD7QJtZr5Jp4TelZex8spn1KpmA35SeFnW7CWRm/SOZgN+klrHzyWbWi2QCvlvGZpa6ZAI+uGVsVkfu8lqepAK+mdVLv3Wm6PeTkwO+mVWmnzpT9NvJqZOk+uFbfk0Yy2D1sbczxSxR+84UTRgD4xa+TVkTWjhWL/3UmaIJPf0c8G3K+unyu2r9nustU790puink1M3Dvg2ZU1o4ZTBV0LN1S8np24c8G3KmtDCKYOvhKyuCrlpK+lkSfdKGpF0UYflb5A0KumO7OecIsq18h21eC7nnnC4A9gE+ulGpKUldwtf0ixgFfASYDtwm6T1EbF53KqfiIjz8pZn5XAOune+ErK6KiKlczQwEhFbAbIXla8Axgd8q5luQd056Pz6PddrzVRESmc+cH/b9PZs3ni/J+kuSVdJWthtY5JWShqSNDQ6OlpA9ayTvUH9Xdffy2vWbHxcv/om9Dc2s32VNfDqP4ElEfE84Abg8m4rRsTqiBiMiMGBgYGSqpeeiYK6c9BmzVRESmcH0N5iX5DN+7mIaG8irgHeUUC5lsNEXSydgzZrpiIC/m3AMkmH0Qr0ZwBnta8g6ZCIeCCbPB3YUkC5lsNkQd05aLPmyR3wI2JM0nnAdcAsYG1EbJL0dmAoItYDfyrpdGAMeBh4Q95yLT8H9fzcm8n6iSKi6jp0NTg4GENDQ1VXw6wj92ayOpI0HBGDnZb5aZlmPXJvJus3DvhmPXJvJus3fpaOWY/cm8n6jQO+WZvp3oT1jW/rJw74ZhnfhLWmcw7fLOObsNZ0DvhmGd+EtaZzSscs45uw1nQO+GZtfBPWmswpHTOzRDjgm5klwgHfzCwRDvhmJRvetotVN4087i1jZmXwTVuzEnlwl1XJLXyzEnlwl1XJAd+sRB7cZVVySsesRB7cZVUqJOBLOhl4H61XHK6JiEvHLT8AuAI4CtgJvDoi7iuibLN+48FdVpXcKR1Js4BVwCnAEcCZko4Yt9ofArsi4nDgPcA/5y3XzMymp4gc/tHASERsjYjdwMeBFePWWQFcnn2+CjhRkgoo28zMpqiIgD8fuL9tens2r+M6ETEGPAJ0vFslaaWkIUlDo6OjBVTPzMyghr10ImJ1RAxGxODAwEDV1bEG8sAnS1URN213AAvbphdk8zqts13SbOAptG7empXKA5+mZ7qvfExZP3xXRQT824Blkg6jFdjPAM4at8564Gzg68ArgS9FRBRQttm0dBr4VNc/zjJ1ClY+OU5dv3xXuQN+RIxJOg+4jla3zLURsUnS24GhiFgPXAZ8RNII8DCtk4JZ6fYOfHpsbI8HPmW6BSufHKeuX76rQvrhR8QGYMO4eW9t+/wT4FVFlDUT+uFSzIr5f/LAp311C1Y+OU5dv3xXyY+07ZdLsdQV+f/kgU+P1y1Y+eQ4df3yXSUf8PvlUix1/n+aORMFK58cp64fvqvkA35Zl2JOG+XTL5fM/aofgpXlpzp3lhkcHIyhoaEZL2emg7HTRsXwSdNscpKGI2Kw07LkW/gw860bpyOK4VaoWT61G2nbRCk/A92jWq1oPqZ65xZ+CfrlDn7RnMqyovmYysct/JIctXgu555weFIHZ1mv83OLLx1+RWQ+buFPwDcJ8ymjZ41bfGlxb618HPC7cCCZnk4nxzJSWb4hnpZU06NFccDvwoFk6iY6Oc50zxq3+NLTyzHlq/UWB/wuHEimrsqTo1t8Nhlfrf+CA34XDiRTV/XJ0f3zH8+t2cfz1fovOOBPwIFkanxyrA+3ZvdVdYOkThzwrRA+OdaDW7P7coPkFxzwzRrErdnO3CBpyRXwJT0V+ASwBLgP+P2I2Gf0i6SfAXdnk/8bEafnKddsKlLMZbs1axPJ28K/CPhiRFwq6aJs+i87rPfjiHh+zrLMpizlXLZbs9ZN3kcrrAAuzz5fDrws5/bMCuEh+Gb7yhvwnx4RD2SfHwSe3mW9J0gakrRR0stylmk2qZSfUGrWzaQpHUk3As/osOiS9omICEnd3qayOCJ2SFoKfEnS3RHx7S7lrQRWAixatGiy6pl15Fy22b5yvfFK0r3A8RHxgKRDgC9HxLMm+Z0PA9dExFWTbb+sN16ZmTXFRG+8ypvSWQ+cnX0+G/hch8LnSjog+zwPOBbYnLNcMzObprwB/1LgJZK+BZyUTSNpUNKabJ1nA0OS7gRuAi6NCAd8M7OS5eqWGRE7gRM7zB8Czsk+fw14bp5yzMwsP7/xymrHb7Aymxl+tILVSsoDpqw+mjpK2wHfasUP/7KqNbnR4ZSO1YoHTFmvikoFNnmUtlv4ViseMGW9KLJV3uQnjjrgW+344V82XUWmApvc6HDAN7O+V3SrvKmNDgd8M+t7TW6VF8kB38waoamt8iK5l46ZWSIc8AvmUaJWZz4+0+aUToGaPGDD+p+PT3MLv0BNHrBh/c/HpzngF8ijRK3OfHxarjdezbR+fONVUx+6ZM3g47P5JnrjlXP4BXPXMKszH59pc0rHzCwRuQK+pFdJ2iRpj6SOlxDZeidLulfSiKSL8pRpZma9ydvCvwd4BXBLtxUkzQJWAacARwBnSjoiZ7mN5/7SZla0vO+03QIgaaLVjgZGImJrtu7HgRWAX2TehftLm9lMKCOHPx+4v216ezavI0krJQ1JGhodHZ3xytWR+0ub2UyYNOBLulHSPR1+VsxEhSJidUQMRsTgwMDATBRRe+4vbWYzYdKUTkSclLOMHcDCtukF2Tzrwo96NbOZUEY//NuAZZIOoxXozwDOKqHcGTXTA1jcX9rMipYr4Et6OfABYAC4VtIdEfHbkg4F1kTEqRExJuk84DpgFrA2IjblrnmFfFPVzPpR3l46VwNXd5j/HeDUtukNwIY8ZdVJke/PTJWH+JuVz49W6EGT32pfBl8hmVXDAb8Hvqmaj6+QzKrhgN8j31Ttna+QzKrhgG+l8xWS1UVq95Ic8K0SvkKyqqV4L8mPRzazJKX4CBMHfDNLUoqPMHFKx6zmUsszlyXFe0kO+GY1lmKeuUyp3UtySsesxlLMM9vMccA3q4lObzlLMc9sM8cpHbMa6Ja6STHPbDPHAd+sBiZ63ERqeWabOU7pmNWAUzdWBrfwzWqgl9SNu2vadDngm9XEdFI37q5pvXBKx6wPubum9SJXwJf0KkmbJO2RNDjBevdJulvSHZKG8pRpZs75W2/ypnTuAV4B/PsU1j0hIr6bszwzI83HAlh+ed9puwVAUjG1MbMpc3dNm66ycvgBXC9pWNLKiVaUtFLSkKSh0dHRkqpnZtZ8k7bwJd0IPKPDoksi4nNTLOfXI2KHpKcBN0j6ZkTc0mnFiFgNrAYYHByMKW7fzMwmMWnAj4iT8hYSETuyfx+SdDVwNNAx4JuZ2cyY8ZSOpCdJOnDvZ+CltG72mplZifJ2y3y5pO3Ai4FrJV2XzT9U0oZstacDX5V0J/AN4NqI+EKecs3MbPry9tK5Gri6w/zvAKdmn7cCR+Ypx8zM8vNIWzOzRDjgV6zTSy/MzGaCH55WIT8Ay8zK5BZ+hfwALDMrkwN+hfwALDMrk1M6FfIDsMysTA74FfMDsMysLE7pmJklwgHfzCwRDvhmZolwwDczS4QDvplZIhzwzcwSoYj6vlRK0iiwbQY2PQ/o5xeq93v9of/3wfWvXr/vw0zVf3FEDHRaUOuAP1MkDUXEYNX16FW/1x/6fx9c/+r1+z5UUX+ndMzMEuGAb2aWiFQD/uqqK5BTv9cf+n8fXP/q9fs+lF7/JHP4ZmYpSrWFb2aWHAd8M7NEJBvwJZ0v6ZuSNkl6R9X16ZWkCyWFpHlV12U6JL0z+/7vknS1pIOqrtNUSDpZ0r2SRiRdVHV9pkvSQkk3SdqcHfsXVF2nXkiaJem/JF1TdV16IekgSVdlfwNbJL24jHKTDPiSTgBWAEdGxHOAf6m4Sj2RtBB4KfC/VdelBzcAvxoRzwP+G/iriuszKUmzgFXAKcARwJmSjqi2VtM2BlwYEUcAy4Fz+3AfAC4AtlRdiRzeB3whIn4FOJKS9iXJgA+8Cbg0In4KEBEPVVyfXr0HeAvQd3feI+L6iBjLJjcCC6qszxQdDYxExNaI2A18nFbDoW9ExAMRcXv2+VFagWZ+tbWaHkkLgN8B1lRdl15IegpwHHAZQETsjojvlVF2qgH/mcBvSLpV0s2SXlR1haZL0gpgR0TcWXVdCvAHwOerrsQUzAfub5veTp8Fy3aSlgAvAG6tuCrT9V5aDZ09FdejV4cBo8B/ZGmpNZKeVEbBjX3FoaQbgWd0WHQJrf1+Kq1L2hcBn5S0NGrWR3WSfbiYVjqntiaqf0R8LlvnElpphnVl1i11kp4MfBp4c0R8v+r6TJWk04CHImJY0vEVV6dXs4EXAudHxK2S3gdcBPxNGQU3UkSc1G2ZpDcBn8kC/Dck7aH1IKPRsuo3Fd32QdJzabUS7pQErXTI7ZKOjogHS6zihCb6PwCQ9AbgNODEup1su9gBLGybXpDN6yuS5tAK9usi4jNV12eajgVOl3Qq8ATglyV9NCJeW3G9pmM7sD0i9l5ZXUUr4M+4VFM6nwVOAJD0TGB/+uipexFxd0Q8LSKWRMQSWgfQC+sU7Ccj6WRal+WnR8SPqq7PFN0GLJN0mKT9gTOA9RXXaVrUaiFcBmyJiHdXXZ/pioi/iogF2XF/BvClPgv2ZH+n90t6VjbrRGBzGWU3toU/ibXAWkn3ALuBs/ukhdkkHwQOAG7IrlI2RsSfVFuliUXEmKTzgOuAWcDaiNhUcbWm61jgdcDdku7I5l0cERuqq1KSzgfWZQ2HrcAbyyjUj1YwM0tEqikdM7PkOOCbmSXCAd/MLBEO+GZmiXDANzNLhAO+mVkiHPDNzBLx/1AhE9y0GtgeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.linspace(-2*np.pi, 2*np.pi, 50)\n",
    "\n",
    "\n",
    "def noisy_sin_wave(x):\n",
    "    return np.sin(x*0.6) + np.random.normal(loc=0, scale=0.1)\n",
    "\n",
    "\n",
    "def noisy_cos_wave(x):\n",
    "    return np.cos(x*2) + np.random.normal(loc=0, scale=0.5)\n",
    "\n",
    "\n",
    "def noisy_tan_wave(x):\n",
    "    return np.tan(x) + np.random.normal(loc=0, scale=0.3)\n",
    "\n",
    "\n",
    "def plot_noisy_func(func, domain,title):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.set_title(title)\n",
    "    ax.plot(domain,np.vectorize(func)(domain), \".\")\n",
    "\n",
    "\n",
    "def create_sample_seq(func1, domain, label):\n",
    "    X = np.array([np.vectorize(func1)(domain)])\n",
    "    Y = np.array([label]*X.shape[0])\n",
    "    X = X.T\n",
    "    return X,Y\n",
    "\n",
    "plot_noisy_func(noisy_sin_wave, X, \"Noisy sin wave\")\n",
    "plot_noisy_func(noisy_cos_wave, X, \"Noisy cos wave\")\n",
    "plot_noisy_func(noisy_tan_wave, X, \"Noisy tan wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}sin+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[sin+e]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_sin, var_cos, var_tan = sy.symbols(\"sin+e, cos+e, tan+e\")\n",
    "sy.Matrix([var_sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}cos+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[cos+e]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix([var_cos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}tan+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[tan+e]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix([var_tan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(domain, sample_size=30):\n",
    "    X0 = [ create_sample_seq(noisy_sin_wave, domain, 0) for _ in range(sample_size)]\n",
    "    X1 = [ create_sample_seq(noisy_cos_wave, domain, 1) for _ in range(sample_size)]\n",
    "    X2 =[ create_sample_seq(noisy_tan_wave, domain, 2) for _ in range(sample_size)]\n",
    "    X = [*X0, *X1 , *X2]\n",
    "    random.shuffle(X)\n",
    "    return X\n",
    "data  = data_loader( X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2842bb0963b34c1889669bf43116807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.002357071238514046\n",
      "Epoch 1 Loss 8.649001121713586e-05\n",
      "Epoch 2 Loss 2.25482266338647e-06\n",
      "Epoch 3 Loss 1.7344027762893394e-08\n",
      "Epoch 4 Loss 1.0233951763311463e-08\n",
      "Epoch 5 Loss nan\n",
      "Epoch 6 Loss nan\n",
      "Epoch 7 Loss nan\n",
      "Epoch 8 Loss nan\n",
      "Epoch 9 Loss nan\n",
      "Epoch 10 Loss nan\n",
      "Epoch 11 Loss nan\n",
      "Epoch 12 Loss nan\n",
      "Epoch 13 Loss 1.0154492212615812e-05\n",
      "Epoch 14 Loss 6.821388441667739e-06\n",
      "Epoch 15 Loss 1.2344867615253419e-05\n",
      "Epoch 16 Loss 1.0570144237627175e-05\n",
      "Epoch 17 Loss 2.0009482299280175e-05\n",
      "Epoch 18 Loss 2.9586066279775306e-05\n",
      "Epoch 19 Loss 3.5491327717903056e-05\n",
      "Epoch 20 Loss 3.8584947598017084e-05\n",
      "Epoch 21 Loss 0.23627207685864687\n",
      "Epoch 22 Loss 0.47319833628032015\n",
      "Epoch 23 Loss 0.48594873224556473\n",
      "Epoch 24 Loss 0.506740564910557\n",
      "Epoch 25 Loss 0.4993856296598219\n",
      "Epoch 26 Loss 0.49621634100710416\n",
      "Epoch 27 Loss 0.510776775004078\n",
      "Epoch 28 Loss 0.7944902378936669\n",
      "Epoch 29 Loss 0.8212609198523448\n",
      "Epoch 30 Loss 0.03428152028376385\n",
      "Epoch 31 Loss 0.038818818854145565\n",
      "Epoch 32 Loss 0.042222214786093046\n",
      "Epoch 33 Loss 0.06932921254567313\n",
      "Epoch 34 Loss 0.09607754920706629\n",
      "Epoch 35 Loss 0.09248771587761492\n",
      "Epoch 36 Loss 0.10039373233800862\n",
      "Epoch 37 Loss 0.11274537525854285\n",
      "Epoch 38 Loss 0.1769245237697812\n",
      "Epoch 39 Loss 0.13586067538809185\n",
      "Epoch 40 Loss 9.163510383646343e-07\n",
      "Epoch 41 Loss 2.681241948933373e-07\n",
      "Epoch 42 Loss 9.768606821357961e-08\n",
      "Epoch 43 Loss 4.160189089422718e-08\n",
      "Epoch 44 Loss 1.7985844542553147e-08\n",
      "Epoch 45 Loss 1.590408168981528e-08\n",
      "Epoch 46 Loss 7.520240217195572e-09\n",
      "Epoch 47 Loss 4.745838722060351e-09\n",
      "Epoch 48 Loss 1.7262250669881788e-09\n",
      "Epoch 49 Loss 2.064401228406677e-09\n",
      "Epoch 50 Loss 7.3236553883602016e-09\n",
      "Epoch 51 Loss 5.063226532971036e-07\n",
      "Epoch 52 Loss 1.6115282395316208e-07\n",
      "Epoch 53 Loss 1.5623225462030747e-08\n",
      "Epoch 54 Loss 1.0457111855328692e-08\n",
      "Epoch 55 Loss 6.6394266574678605e-09\n",
      "Epoch 56 Loss 3.88538334032343e-09\n",
      "Epoch 57 Loss 2.415451888219026e-09\n",
      "Epoch 58 Loss 1.4253191746482093e-09\n",
      "Epoch 59 Loss 1.0199180401040927e-09\n",
      "Epoch 60 Loss 8.35686525299582e-10\n",
      "Epoch 61 Loss 8.87304296685757e-10\n",
      "Epoch 62 Loss 9.122750377146053e-10\n",
      "Epoch 63 Loss 9.226537370949213e-10\n",
      "Epoch 64 Loss 9.800217651209435e-10\n",
      "Epoch 65 Loss 1.0456941086566912e-09\n",
      "Epoch 66 Loss 1.1196609058518747e-09\n",
      "Epoch 67 Loss 1.2020694290755333e-09\n",
      "Epoch 68 Loss 1.292794116885731e-09\n",
      "Epoch 69 Loss 1.392028703098647e-09\n",
      "Epoch 70 Loss nan\n",
      "Epoch 71 Loss nan\n",
      "Epoch 72 Loss nan\n",
      "Epoch 73 Loss nan\n",
      "Epoch 74 Loss nan\n",
      "Epoch 75 Loss nan\n",
      "Epoch 76 Loss nan\n",
      "Epoch 77 Loss nan\n",
      "Epoch 78 Loss nan\n",
      "Epoch 79 Loss nan\n",
      "Epoch 80 Loss nan\n",
      "Epoch 81 Loss nan\n",
      "Epoch 82 Loss nan\n",
      "Epoch 83 Loss nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m X:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=11'>12</a>\u001b[0m     x,y  \u001b[39m=\u001b[39m pair        \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=12'>13</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mLoss(x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=13'>14</a>\u001b[0m     model\u001b[39m.\u001b[39mstep(x, y, lr\u001b[39m=\u001b[39mLR)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(x)\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 31'\u001b[0m in \u001b[0;36mRNN.Loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=111'>112</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoss\u001b[39m(\u001b[39mself\u001b[39m, x,y):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=112'>113</a>\u001b[0m     o,h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)      \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=113'>114</a>\u001b[0m     yHat\u001b[39m=\u001b[39m o[\u001b[39mlen\u001b[39m(x)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=114'>115</a>\u001b[0m     y_1h \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(yHat)\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 31'\u001b[0m in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=42'>43</a>\u001b[0m h[t] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m x[t] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=44'>45</a>\u001b[0m     h[t] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW \u001b[39m@\u001b[39;49m h[t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=45'>46</a>\u001b[0m h[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(h[t])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=46'>47</a>\u001b[0m o[t] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m h[t] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "MAX_EPOCHS = 1000\n",
    "LR=0.001\n",
    "\n",
    "model = RNN(input_dim=1, output_dim=3, hidden_dim=128)\n",
    "domain = np.linspace(-2*np.pi, 2*np.pi, 50)\n",
    "X = data_loader(domain)\n",
    "\n",
    "for epoch in tqdm(range(MAX_EPOCHS)):\n",
    "    loss = 0\n",
    "    for pair in X:\n",
    "        x,y  = pair        \n",
    "        loss += model.Loss(x, y)\n",
    "        model.step(x, y, lr=LR)\n",
    "        loss = loss / len(x)\n",
    "    print(f\"Epoch {epoch} Loss {loss}\")\n",
    "    loss_history.append(loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
