{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "import sympy as sy\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},\\theta)  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta \\text{: parameters shared across all time steps}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, its state at time step t, is dependent only on a set a parameters and the previous state at t-1\n",
    "<br>\n",
    "<br>\n",
    "Let the state of the system, h, also be depedent on an input at the respective time step, x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},x^{t},\\theta)  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state h now contains information about the entire past history of inputs, x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a system that given the hidden state, h,produces an output o, for each time step. This output is passed to an activation function made to predict the target, y, at the respective time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "o^t= g (h^{t},\\theta')  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta' \\text{: a different set of parameters as $\\theta$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now define $\\theta$ and $\\theta'$ as the weight matrices describing the relation between the input-to-hidden, hidden-to-hidden and hidden-to-output notes; $U$, $W$ and $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z^t=  W^{T}h^{t-1} + U^{T}x^t +b \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{t} = \\phi(z^t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o^t = V^Th^{t} + c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $b$ and $c$ are biases, $\\phi$ is an activation function. <br><br>\n",
    "**Note**: matrices $U$, $W$ and $V$ are not indexed by time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each time step, we have a sequential total loss up to time step $\\tau$, $L^\\tau$, defined as the difference between our prediction and the target, at each output, upto the time step $\\tau$\n",
    "<br>\n",
    "<br>\n",
    "Consider the task of multi-class classification. \n",
    "<br>\n",
    "<br>\n",
    "Consequently, the output activation function is the normalized expontential function, a.k.a the _softmax function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\sum_{t=1}^{\\tau} l\\big(o^{t}\\big)\n",
    "\\text{: Total loss upto time step $\\tau$}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\text{: Softmax activation function for multi-class classification}\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE** the softmax is a vector function, later when taking the derivative, in reality I am finding the Jacobian of it in its vector form, but here I denote one element of it, the $i^{th}$\n",
    "\n",
    "\\begin{equation}\n",
    "l = - \\sum_{m=0}^{M-1}y_{m}^{t} \\log\\Big(\\hat{y}_{m}^{t}\\Big)\n",
    "\\text{: M categorical cross entropy for predictions at time step $t$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process differs from standard back-propagation (like descirbed for a vanilla feedforward network). Usng the above assumptions, I will go through the derivation analogous optimization process for recurrent networks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation through time\n",
    "\n",
    "Per example loss w.r.t to the output element $o_i$ at time $t$; $o_i^{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{o_{i}^{t}} L = \\frac{\\partial{L}}{\\partial{l(o_i^t)}} \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "Note that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{L}}{\\partial{l(o_i^t)}} = 1\n",
    "\\end{equation}\n",
    "and that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "is the derivative of the categorical cross-entropy\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}}\\frac{\\partial{\\hat{y}^{t}_j}}{\\partial{o_i^{t}}} } - [1]\n",
    "\\end{equation}\n",
    "The softmax functions is:\n",
    " \\begin{equation}\n",
    " \\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\end{equation}\n",
    "Taking its derivative gives:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "    \\frac{\\partial{\\hat{y}^{t}_i}}{\\partial{o_j^{t}}} = \\hat{y}^{t}_{i} \\Big( \\delta_{ij}  -  \\hat{y}^{t}_{j} \\Big)\n",
    "}- [2]\n",
    "\\end{equation}\n",
    "_look at the different cases to see why this is true_ i.e. $i=j$ and $i \\neq j$\n",
    "<br><br>\n",
    "Lets sub [2] into [1], and splitting into the cases where $i=j$ and $i \\neq j $:\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}} \\hat{y}^{t}_{j} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    " \n",
    "Lets now split the sum up for the two cases;\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j=i} + \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j \\neq i}  =  -y^{t}_{i}(\\delta_{ii} - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "Simplfying down: \n",
    "\n",
    "\\begin{equation}\n",
    " \n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big( 0 -\\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} \n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}+y^{t}_{i}\\hat{y}_{i}^{t}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "Recall that $\\sum_{j} y_j = 1$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} \\Big( y_j^{t} +y^{t}_{i} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j} \\Big( y_j^{t} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets calculate the gradient on the internel nodes $h^t$ from the end of the sequence $\\tau$.\n",
    "<br>\n",
    "I am going to use vector notation here on out. I.e. $h_i^{t}$ becomes $h^t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = \\Bigg( \\frac{ \\partial{o^{\\tau}}}\n",
    "{\\partial{h^{\\tau}}} \\Bigg)^{T} \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = V \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "we iterate backwards through time. Note the dependency of $h^t$ on both $o^t$ and $h^{t+1}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = \\Bigg( \\frac{ \\partial{h^{t+1}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{h^{t+1}} L +\n",
    "\\Bigg( \\frac{ \\partial{o^{t}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{o^{t}} L \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate of the hidden units  w.r.t their previous time step is:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  \\frac{ \\partial{h^{t+1}} }{ \\partial{z^{t+1} } }\n",
    "\\frac{ \\partial{z^{t+1} } } { \\partial{h^{t}} }\n",
    "\\end{equation}\n",
    "This leads to:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  diag\\Bigg( \\phi'\\big(z^{t+1}\\big) \\Bigg) W^T\n",
    "\\end{equation}\n",
    "**Note** diag: considering only the leading diagonal values and setting all others to 0. \n",
    "<br><br>\n",
    "For RNNs , we want to use a saturating activation to avoid gradient explosions <br><br>\n",
    "e.g. hyperbolic tagent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \\phi'\\big(z^{t+1}\\big) \\Big)   \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets specify the activation function (using the hyperpolic tagent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \n",
    "     1 - \\big(h^{t+1}\\big)^2\n",
    "    \\Big)  \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradients on the biases $b$ and $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\Bigg(\n",
    "     \\frac{\\partial{o^t}}{\\partial{c^t}} \n",
    "     \\Bigg)^{T} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "since $\\frac{\\partial{o^t}}{\\partial{c^t}} = 1$\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  \\Bigg(\n",
    "     \\frac{\\partial{h^t}}{\\partial{b^t}} \n",
    "     \\Bigg)^{T}  \\nabla_{h^t} L\n",
    "\\end{equation}\n",
    "Since $b$ is dependent on h through the activation function $\\phi$, we have: \n",
    "\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  diag \\Bigg( \\phi' \\Big( z^t \\Big) \\Bigg) \\nabla_{h^t} L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative w.r.t to $V$; the hidden-ouput matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{V} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial o_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{V} O_i^{t}\n",
    "\\end{equation}\n",
    "Leading to:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{V} L  = \\sum_{t} h^t \\Big(\\nabla_{o^t} L \\Big)^T\n",
    "}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative w.r.t the weight matrices $W$ and $U$, we introduce dummy variables $W^t$ and $U^t$. These are copies of each other at each time step, summing these up will give us the total gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{W} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{W^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{W} L  = \\sum_{t} h^{t-1} \\Big(\\nabla_{h^t} L \\Big)^T  diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n",
    "for the derivative of w.r.t $U$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{U} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{U^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{U} L  = \\sum_{t} x^{t} \\Big( \\nabla_{h^t} L \\Big)^T \n",
    "     diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling RNN with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        # network variables \n",
    "        self.idim = input_dim\n",
    "        self.hdim = hidden_dim\n",
    "        self.odim = output_dim\n",
    "        # initialise weights \n",
    "        self.U = np.random.uniform(- np.sqrt(1./self.idim),\n",
    "                                     np.sqrt(1./self.idim),\n",
    "                                    (self.idim, self.hdim) )\n",
    "\n",
    "        self.V = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.odim))\n",
    "\n",
    "        self.W = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.hdim))\n",
    "\n",
    "        self.b = np.zeros(self.hdim)\n",
    "        self.c = np.zeros(self.odim)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        Note that this is a numerically stable version of softmax.\n",
    "        We substract the max value from all elements.\n",
    "        Overflow of a single element, or underflow of all elements,  will render the output usless.\n",
    "        subtracting max leaves only non-positive values ---> no overflow \n",
    "        at least one element = 0 ---> no vanishing denominator (underflow is some enteries is okay) \n",
    "         '''\n",
    "        xt = np.exp(x-np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Single example pass forward, all the way through the network\n",
    "        T = len(x)\n",
    "        # will stack as rows\n",
    "        h = np.zeros((T,self.hdim))\n",
    "        o = np.zeros((T,self.odim))\n",
    "        for t in range(T):\n",
    "            h[t] = self.U.T @ x[t] + self.b\n",
    "            if t > 1:\n",
    "                h[t] += self.W @ h[t-1] + self.b\n",
    "            h[t] = np.tanh(h[t])\n",
    "            o[t] = self.softmax( self.V.T @ h[t] + self.c)\n",
    "        return (o,h)\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, x, y, clip=None):\n",
    "        T = len(x)\n",
    "        o,h = self.forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdb = np.zeros(self.b.shape)\n",
    "        dLdc = np.zeros(self.c.shape)\n",
    "        # dL/do\n",
    "        delta_o = o\n",
    "        # Notice, only evaluting at last output of network, yHat - y \n",
    "        delta_o[ np.arange(T), y ] -= float(y)\n",
    "\n",
    "        # use output from the last time step, \n",
    "        # dL/dh\n",
    "        delta_h = np.zeros((T, self.hdim))\n",
    "        for t in reversed(range(T)):\n",
    "            # collect errors on hidden states\n",
    "            delta_h[t] = self.V @ delta_o[T-1,:]\n",
    "            if t < T-1:\n",
    "                # collect errors on hidden states due to W\n",
    "                delta_h[t] = ( self.W @ np.diag(1-h[t+1]**2) ) @ delta_h[t+1]\n",
    "        for t in range(T):\n",
    "            # error on ouput bias\n",
    "            dLdc += delta_o[T-1,:]\n",
    "            # error on hidden bias \n",
    "            dLdb += (1-h[t]**2) * delta_h[t,:]\n",
    "            # error on hidden-output matrix\n",
    "            ot = delta_o[T-1,:][...,np.newaxis]\n",
    "            ht = h[t,:][...,np.newaxis]\n",
    "            dht = delta_h[t,:][...,np.newaxis]\n",
    "\n",
    "            dLdV += ht @ ot.T \n",
    "\n",
    "            # error on hidden-hidden W\n",
    "            if t > 0 :\n",
    "                h_t = h[t-1,:][...,np.newaxis]\n",
    "                dLdW += ( h_t @ dht.T )@np.diag(1-h[t]**2)\n",
    "\n",
    "            xt = x[t][...,np.newaxis]\n",
    "            dLdU += xt @ dht.T @ np.diag(1-h[t]**2)\n",
    "\n",
    "        if clip is not None:\n",
    "            dLdb = np.clip(dLdb, -clip, clip)\n",
    "            dLdc = np.clip(dLdc, -clip, clip)\n",
    "            dLdV = np.clip(dLdV, -clip, clip)\n",
    "            dLdW = np.clip(dLdW, -clip, clip)\n",
    "            dLdU = np.clip(dLdU, -clip, clip)\n",
    "        return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
    "\n",
    "\n",
    "    def step(self,x,y,lr=0.0001):\n",
    "        dLdU, dLdV, dLdW, dLdb, dLdc = self.backward(x,y)\n",
    "        self.U -= lr * dLdU\n",
    "        self.V -= lr * dLdV\n",
    "        self.W -= lr * dLdW \n",
    "        self.b -= lr * dLdb \n",
    "        self.c -= lr * dLdc \n",
    "    \n",
    "\n",
    "    def Loss(self, x,y):\n",
    "        o,h = self.forward(x)      \n",
    "        yHat= o[len(x)-1, :]\n",
    "        y_1h = [0.0]*len(yHat)\n",
    "        y_1h[int(y)] = 1.0\n",
    "        LOSS = self.categorical_cross_entropy_loss(yHat, y_1h)\n",
    "        return LOSS\n",
    "\n",
    "\n",
    "    def categorical_cross_entropy_loss(self, yHats, ys):\n",
    "        loss = 0.0\n",
    "        for pred,true in zip(yHats, ys):\n",
    "            loss += -1.0*(true * np.log(pred) + (1.0-true) * np.log(1.0 - pred))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling synthetic time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.linspace(-2*np.pi, 2*np.pi, 50)\n",
    "\n",
    "\n",
    "def noisy_sin_wave(x):\n",
    "    return np.sin(x*0.6) + np.random.normal(loc=0, scale=0.1)\n",
    "\n",
    "\n",
    "def noisy_cos_wave(x):\n",
    "    return np.cos(x*2) + np.random.normal(loc=0, scale=0.5)\n",
    "\n",
    "\n",
    "def noisy_tan_wave(x):\n",
    "    return np.tan(x) + np.random.normal(loc=0, scale=0.3)\n",
    "\n",
    "\n",
    "def plot_noisy_func(func, domain,title):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.set_title(title)\n",
    "    ax.plot(domain,np.vectorize(func)(domain), \".\")\n",
    "\n",
    "\n",
    "def create_sample_seq(func1, domain, label):\n",
    "    X = np.array([np.vectorize(func1)(domain)])\n",
    "    Y = np.array([label]*X.shape[0])\n",
    "    X = X.T\n",
    "    return X,Y\n",
    "\n",
    "plot_noisy_func(noisy_sin_wave, X, \"Noisy sin wave\")\n",
    "plot_noisy_func(noisy_cos_wave, X, \"Noisy cos wave\")\n",
    "plot_noisy_func(noisy_tan_wave, X, \"Noisy tan wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}sin+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[sin+e]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_sin, var_cos, var_tan = sy.symbols(\"sin+e, cos+e, tan+e\")\n",
    "sy.Matrix([var_sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}cos+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[cos+e]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix([var_cos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}tan+e\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[tan+e]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix([var_tan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(domain, sample_size=30):\n",
    "    X0 = [ create_sample_seq(noisy_sin_wave, domain, 0) for _ in range(sample_size)]\n",
    "    X1 = [ create_sample_seq(noisy_cos_wave, domain, 1) for _ in range(sample_size)]\n",
    "    X2 =[ create_sample_seq(noisy_tan_wave, domain, 2) for _ in range(sample_size)]\n",
    "    X = [*X0, *X1 , *X2]\n",
    "    random.shuffle(X)\n",
    "    return X\n",
    "data  = data_loader( X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac71ebd0a924bbdaea091cf6f6b8536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in loop 2.651692522933213\n",
      "loss in loop 2.4336898719738373\n",
      "loss in loop 2.119753023358544\n",
      "loss in loop 1.9425576367231607\n",
      "loss in loop 2.476346155758288\n",
      "loss in loop 1.6645609642900068\n",
      "loss in loop 1.5262533959246651\n",
      "loss in loop 2.7130774620064164\n",
      "loss in loop 1.7970061123506957\n",
      "loss in loop 1.8897458447383852\n",
      "loss in loop 1.6870525066933406\n",
      "loss in loop 2.6788790429583873\n",
      "loss in loop 1.611074103367781\n",
      "loss in loop 1.5763161611428558\n",
      "loss in loop 1.834159035947062\n",
      "loss in loop 1.634857961806856\n",
      "loss in loop 2.9327736042318984\n",
      "loss in loop 2.989706985497531\n",
      "loss in loop 1.627785735476077\n",
      "loss in loop 2.157003533152071\n",
      "loss in loop 2.919430891586603\n",
      "loss in loop 1.9250382786489657\n",
      "loss in loop 1.8294799419586498\n",
      "loss in loop 1.4497877857653518\n",
      "loss in loop 2.856926990964438\n",
      "loss in loop 2.856942576486416\n",
      "loss in loop 2.7985046455045386\n",
      "loss in loop 2.8039131417838448\n",
      "loss in loop 1.0237698031111857\n",
      "loss in loop 1.0672900789420663\n",
      "loss in loop 3.0914426000562423\n",
      "loss in loop 1.7029110375809173\n",
      "loss in loop 0.7955494445111689\n",
      "loss in loop 0.6993206177829326\n",
      "loss in loop 3.1803543590922874\n",
      "loss in loop 3.423446533967316\n",
      "loss in loop 3.23874687701491\n",
      "loss in loop 1.0610256802597369\n",
      "loss in loop 0.8286717142220732\n",
      "loss in loop 1.3687755551605554\n",
      "loss in loop 3.3331105146512767\n",
      "loss in loop 3.717101031621156\n",
      "loss in loop 0.9105648052570617\n",
      "loss in loop 3.8344311251596057\n",
      "loss in loop 1.1270303001891833\n",
      "loss in loop 0.6729747134513072\n",
      "loss in loop 1.27356794910379\n",
      "loss in loop 4.263930983568748\n",
      "loss in loop 3.842188831739614\n",
      "loss in loop 4.19715119545395\n",
      "loss in loop 1.3574602235311397\n",
      "loss in loop 0.44029045451630794\n",
      "loss in loop 1.041169634968852\n",
      "loss in loop 0.7896200730569467\n",
      "loss in loop 0.5861871916719524\n",
      "loss in loop 0.8714691119481535\n",
      "loss in loop 1.1252752569212119\n",
      "loss in loop 1.3552832328069233\n",
      "loss in loop 0.6475577639037366\n",
      "loss in loop 1.1341043666220088\n",
      "loss in loop 0.8225136505895901\n",
      "loss in loop 0.833559562078145\n",
      "loss in loop 0.3505153571790698\n",
      "loss in loop 0.8014296592031103\n",
      "loss in loop 0.4821676383143395\n",
      "loss in loop 5.500523152133909\n",
      "loss in loop 5.230479219758987\n",
      "loss in loop 5.436194660568459\n",
      "loss in loop 1.4238725160091632\n",
      "loss in loop 0.6536909162534672\n",
      "loss in loop 0.2073562019624638\n",
      "loss in loop 5.7573849529725205\n",
      "loss in loop 0.4772609845363842\n",
      "loss in loop 1.1082611383738077\n",
      "loss in loop 0.15056375308013772\n",
      "loss in loop 1.2182502253008347\n",
      "loss in loop 6.790870521955635\n",
      "loss in loop 5.270588049248238\n",
      "loss in loop 0.306466488205659\n",
      "loss in loop 0.9254430111518306\n",
      "loss in loop 0.12234325173715205\n",
      "loss in loop 0.6144720488599048\n",
      "loss in loop 0.06362306729869009\n",
      "loss in loop 6.010890305663495\n",
      "loss in loop 1.6081101583331823\n",
      "loss in loop 0.9629319577261163\n",
      "loss in loop 6.779816160765424\n",
      "loss in loop 6.315228997089166\n",
      "loss in loop 1.3573647475318635\n",
      "loss in loop 6.670632999309852\n",
      "Epoch 0 Loss 0.13341265998619703\n",
      "loss in loop 1.060910499000262\n",
      "loss in loop 0.9484275550648628\n",
      "loss in loop 0.8016770633829429\n",
      "loss in loop 0.10679322141010117\n",
      "loss in loop 5.991187934574142\n",
      "loss in loop 0.1504337413954347\n",
      "loss in loop 0.05503076341436099\n",
      "loss in loop 6.819071935792284\n",
      "loss in loop 0.7273617304322346\n",
      "loss in loop 0.7763085793478745\n",
      "loss in loop 0.5901019464841254\n",
      "loss in loop 6.717138605064137\n",
      "loss in loop 0.6612135097743443\n",
      "loss in loop 0.5658226131004865\n",
      "loss in loop 1.0268337480941903\n",
      "loss in loop 0.41208543713084783\n",
      "loss in loop 7.731787198439651\n",
      "loss in loop 8.089188080799437\n",
      "loss in loop 0.8907796229861158\n",
      "loss in loop 5.213795435923416\n",
      "loss in loop 6.9764078864934875\n",
      "loss in loop 1.6735741167243317\n",
      "loss in loop 0.881720079899102\n",
      "loss in loop 0.7754389787556136\n",
      "loss in loop 6.560651597556076\n",
      "loss in loop 7.29048289983241\n",
      "loss in loop 5.951768408883878\n",
      "loss in loop 5.952326561420186\n",
      "loss in loop 0.14222591935965956\n",
      "loss in loop 0.0707385771216971\n",
      "loss in loop 6.814425092022957\n",
      "loss in loop 1.6011041662428107\n",
      "loss in loop 0.051527705785370165\n",
      "loss in loop 0.022746196908032867\n",
      "loss in loop 5.833922380298896\n",
      "loss in loop 7.002921788445328\n",
      "loss in loop 5.922397136100647\n",
      "loss in loop 0.5420028095509489\n",
      "loss in loop 0.07858881774416071\n",
      "loss in loop 0.6181189618624096\n",
      "loss in loop 5.761396403740609\n",
      "loss in loop 7.512278725873143\n",
      "loss in loop 0.230222724254231\n",
      "loss in loop 7.662956223102142\n",
      "loss in loop 0.36506123172100324\n",
      "loss in loop 0.06407174226327966\n",
      "loss in loop 0.6294960940277483\n",
      "loss in loop 8.19786942993972\n",
      "loss in loop 6.077216772520639\n",
      "loss in loop 7.8969294285669065\n",
      "loss in loop 0.7760029669678609\n",
      "loss in loop 0.02966843942800345\n",
      "loss in loop 0.48332405589618166\n",
      "loss in loop 0.37208729442394056\n",
      "loss in loop 0.051298018695860155\n",
      "loss in loop 0.1320818568763741\n",
      "loss in loop 0.5197830105157071\n",
      "loss in loop 0.5014670258620736\n",
      "loss in loop 0.07790284190762962\n",
      "loss in loop 0.5637693029282096\n",
      "loss in loop 0.3876497990819475\n",
      "loss in loop 0.39359865485214446\n",
      "loss in loop 0.018356113042302013\n",
      "loss in loop 0.07606704523979484\n",
      "loss in loop 0.036384780983618165\n",
      "loss in loop 8.323202641399122\n",
      "loss in loop 7.229499418282265\n",
      "loss in loop 7.4587803505629005\n",
      "loss in loop 1.680065652213286\n",
      "loss in loop 0.06952909170325103\n",
      "loss in loop 0.004514545816677105\n",
      "loss in loop 8.929320649304685\n",
      "loss in loop 0.19204504800729733\n",
      "loss in loop 0.6894580392758367\n",
      "loss in loop 0.01505844056696724\n",
      "loss in loop 1.3233140533738477\n",
      "loss in loop 10.105753507507103\n",
      "loss in loop 4.856713703259146\n",
      "loss in loop 0.09925137657643035\n",
      "loss in loop 0.4321949161934261\n",
      "loss in loop 0.009223727135714965\n",
      "loss in loop 0.01416064407460338\n",
      "loss in loop 0.0004567932455721489\n",
      "loss in loop 3.9603280771736697\n",
      "loss in loop 3.7111781076803174\n",
      "loss in loop 0.7968679698655867\n",
      "loss in loop 5.607095938403017\n",
      "loss in loop 4.290103865436549\n",
      "loss in loop 0.9158988356327357\n",
      "loss in loop 4.721083078077677\n",
      "Epoch 1 Loss 0.09442166156155354\n",
      "loss in loop 0.7001160236674084\n",
      "loss in loop 0.4638082617326927\n",
      "loss in loop 0.4275397573829834\n",
      "loss in loop 0.008739600250142828\n",
      "loss in loop 3.4137865624921777\n",
      "loss in loop 0.06830867709917436\n",
      "loss in loop 0.0014019153824125963\n",
      "loss in loop 3.369643063398107\n",
      "loss in loop 0.445418019131472\n",
      "loss in loop 0.8719429647648498\n",
      "loss in loop 0.3778879001194763\n",
      "loss in loop 3.3469923736863496\n",
      "loss in loop 0.45842768919333826\n",
      "loss in loop 0.3756685638730031\n",
      "loss in loop 0.009593337400259375\n",
      "loss in loop 0.0003922947017562454\n",
      "loss in loop 3.3607408381075685\n",
      "loss in loop 3.3158760440033106\n",
      "loss in loop 4.28292273822636\n",
      "loss in loop 20.357709157646553\n",
      "loss in loop 3.8375111701836877\n",
      "loss in loop 0.0834688098982225\n",
      "loss in loop 0.4495444864309367\n",
      "loss in loop 2.249141066601259\n",
      "loss in loop 3.475694424201873\n",
      "loss in loop 3.4049184656862783\n",
      "loss in loop 3.358231001031378\n",
      "loss in loop 3.306966098139972\n",
      "loss in loop 0.06614413583381198\n",
      "loss in loop 0.0013311809312305932\n",
      "loss in loop 3.0977624135546638\n",
      "loss in loop 0.5851834064604247\n",
      "loss in loop 0.011704915081242877\n",
      "loss in loop 0.00023495445626728695\n",
      "loss in loop 2.9378153381159002\n",
      "loss in loop 2.9973522330223625\n",
      "loss in loop 2.924751909292891\n",
      "loss in loop 0.4909469366864673\n",
      "loss in loop 0.00982053724148976\n",
      "loss in loop 5.732689783264041\n",
      "loss in loop 2.921194025948735\n",
      "loss in loop 3.1202767601846206\n",
      "loss in loop 0.06241031033547381\n",
      "loss in loop 2.7499313971408585\n",
      "loss in loop 0.05500430937975154\n",
      "loss in loop 0.0011006136452247892\n",
      "loss in loop 5.553410633383423\n",
      "loss in loop 2.722389718306903\n",
      "loss in loop 2.5093233324868187\n",
      "loss in loop 2.461880992724535\n",
      "loss in loop 0.8988233008634594\n",
      "loss in loop 0.0179765149111952\n",
      "loss in loop 0.606109713092866\n",
      "loss in loop 0.3984313690520064\n",
      "loss in loop 0.007968715349998684\n",
      "loss in loop 0.00015941115099180923\n",
      "loss in loop 4.832909453333177\n",
      "loss in loop 0.0966583726345096\n",
      "loss in loop 0.001933177265050139\n",
      "loss in loop 0.5296765986123393\n",
      "loss in loop 0.5754171151380306\n",
      "loss in loop 0.6295130396184443\n",
      "loss in loop 0.012590261097253115\n",
      "loss in loop 0.0002518062751802283\n",
      "loss in loop 5.036275063635598e-06\n",
      "loss in loop 2.641928475176306\n",
      "loss in loop 2.587083066820758\n",
      "loss in loop 2.4368768452145475\n",
      "loss in loop 49.447322532386366\n",
      "loss in loop 0.9889464509250784\n",
      "loss in loop 0.019778929033443728\n",
      "loss in loop 1.5378425911110933\n",
      "loss in loop 0.030756851846334577\n",
      "loss in loop 37.397849407122436\n",
      "loss in loop 9.228080816144887\n",
      "loss in loop 0.9295799040815467\n",
      "loss in loop 3.133236472494103\n",
      "loss in loop 3.4995241016205023\n",
      "loss in loop 7.099729502553603\n",
      "loss in loop 1.1546602129301107\n",
      "loss in loop 6.731264933329239\n",
      "loss in loop 4.168634294019945\n",
      "loss in loop 6.119692161931479\n",
      "loss in loop 3.3578901224344215\n",
      "loss in loop 4.4931702790168195\n",
      "loss in loop 3.9964022218390873\n",
      "loss in loop 3.1886904763786044\n",
      "loss in loop 3.0719956668390425\n",
      "loss in loop 2.126852334664867\n",
      "loss in loop 3.185683938655375\n",
      "Epoch 2 Loss 0.06371367877310749\n",
      "loss in loop 4.343291724091536\n",
      "loss in loop 2.3835179304073817\n",
      "loss in loop 1.6006419839683148\n",
      "loss in loop 5.554833924913579\n",
      "loss in loop 3.217415303859701\n",
      "loss in loop 5.727451298230402\n",
      "loss in loop 4.702705180432082\n",
      "loss in loop 3.203492455816588\n",
      "loss in loop 6.973103200280392\n",
      "loss in loop 3.5966515924985223\n",
      "loss in loop 4.09471028909413\n",
      "loss in loop 2.8180407262052474\n",
      "loss in loop 3.65728568235333\n",
      "loss in loop 1.6630612682051389\n",
      "loss in loop 3.25783621459415\n",
      "loss in loop 3.810130941078168\n",
      "loss in loop 2.3716199598567447\n",
      "loss in loop 2.2029271434693896\n",
      "loss in loop 2.60681738508197\n",
      "loss in loop 0.5602681333101922\n",
      "loss in loop 2.46773688173239\n",
      "loss in loop 2.1070701801106266\n",
      "loss in loop 1.9746871951153668\n",
      "loss in loop 1.3965929095430736\n",
      "loss in loop 2.252615219818117\n",
      "loss in loop 2.3848220250594765\n",
      "loss in loop 2.640189727395001\n",
      "loss in loop 2.546164355903028\n",
      "loss in loop 3.335666021589586\n",
      "loss in loop 2.202365335618065\n",
      "loss in loop 2.388491470110016\n",
      "loss in loop 1.226697273021504\n",
      "loss in loop 1.6587201074378097\n",
      "loss in loop 1.4339437991924247\n",
      "loss in loop 3.0135936173316957\n",
      "loss in loop 2.8016225611991805\n",
      "loss in loop 3.138794805486391\n",
      "loss in loop 7.542418406499771\n",
      "loss in loop 1.0190139485410328\n",
      "loss in loop 5.433466528999937\n",
      "loss in loop 3.347235045201935\n",
      "loss in loop 2.632874574083303\n",
      "loss in loop 0.48850030345800777\n",
      "loss in loop 2.8476996557313243\n",
      "loss in loop 0.23059867861807246\n",
      "loss in loop 0.133158654622072\n",
      "loss in loop 3.0726668652108664\n",
      "loss in loop 2.5816102442765185\n",
      "loss in loop 3.684257350061133\n",
      "loss in loop 3.3272379076448533\n",
      "loss in loop 4.888332697431064\n",
      "loss in loop 0.12563875956342735\n",
      "loss in loop 2.188125290794536\n",
      "loss in loop 1.4273905982533266\n",
      "loss in loop 0.042437932724022215\n",
      "loss in loop 0.027862519912539786\n",
      "loss in loop 0.7029297845281809\n",
      "loss in loop 0.03315335654847263\n",
      "loss in loop 0.004592607946522054\n",
      "loss in loop 5.270744742430774\n",
      "loss in loop 0.5178247143757905\n",
      "loss in loop 0.26191250149756734\n",
      "loss in loop 0.005653098320124588\n",
      "loss in loop 0.0006212078847840015\n",
      "loss in loop 0.00016818065154013478\n",
      "loss in loop 5.733669672001967\n",
      "loss in loop 5.775099671688489\n",
      "loss in loop 5.334014333639878\n",
      "loss in loop 3.4087876997702478\n",
      "loss in loop 0.06819646171480769\n",
      "loss in loop 0.0013668001223604638\n",
      "loss in loop 4.894057401050348\n",
      "loss in loop 0.09788524207541648\n",
      "loss in loop 2.2447614577261583\n",
      "loss in loop 0.04489543523241034\n",
      "loss in loop 11.70502444835629\n",
      "loss in loop 4.895780655770699\n",
      "loss in loop 5.550841787854776\n",
      "loss in loop 0.11101691625490584\n",
      "loss in loop 0.1646821542469468\n",
      "loss in loop 0.0032936600209873316\n",
      "loss in loop 6.590911601782203e-05\n",
      "loss in loop 1.319569217620505e-06\n",
      "loss in loop 5.325594818006631\n",
      "loss in loop 23.542574888737995\n",
      "loss in loop 4.061934182340182\n",
      "loss in loop 5.678757683708326\n",
      "loss in loop 5.273915449863019\n",
      "loss in loop 0.4040247324251236\n",
      "loss in loop 4.446519245993768\n",
      "Epoch 3 Loss 0.08893038491987536\n",
      "loss in loop 4.542531458650768\n",
      "loss in loop 0.21669744768526278\n",
      "loss in loop 0.20565365227343144\n",
      "loss in loop 0.004113073164995573\n",
      "loss in loop 6.096385502430124\n",
      "loss in loop 0.12192771008045256\n",
      "loss in loop 0.002438554210047967\n",
      "loss in loop 5.25464924965953\n",
      "loss in loop 0.4493319055570057\n",
      "loss in loop 0.9058215391545794\n",
      "loss in loop 0.11985094971457175\n",
      "loss in loop 6.0650188090870465\n",
      "loss in loop 0.3229824111524143\n",
      "loss in loop 0.16536543724943767\n",
      "loss in loop 0.0033073088214161735\n",
      "loss in loop 6.614617810198468e-05\n",
      "loss in loop 6.034229431125119\n",
      "loss in loop 5.146060668975389\n",
      "loss in loop 27.920000795945295\n",
      "loss in loop 62.08712881559232\n",
      "loss in loop 8.262065735381134\n",
      "loss in loop 0.16524131471304013\n",
      "loss in loop 0.13334421453847525\n",
      "loss in loop 0.09255072078604575\n",
      "loss in loop 6.823493046086092\n",
      "loss in loop 5.7810525033069835\n",
      "loss in loop 4.76968167999357\n",
      "loss in loop 3.916220695868818\n",
      "loss in loop 0.07832441391738469\n",
      "loss in loop 0.0015664882783493592\n",
      "loss in loop 2.8256816742370603\n",
      "loss in loop 1.777786436720298\n",
      "loss in loop 0.035555728734406294\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n",
      "loss in loop nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=12'>13</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mLoss(x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss in loop\u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=14'>15</a>\u001b[0m     model\u001b[39m.\u001b[39;49mstep(x, y, lr\u001b[39m=\u001b[39;49mLR)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=15'>16</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000038?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 31'\u001b[0m in \u001b[0;36mRNN.step\u001b[0;34m(self, x, y, lr)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=102'>103</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m,x,y,lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=103'>104</a>\u001b[0m     dLdU, dLdV, dLdW, dLdb, dLdc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(x,y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=104'>105</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dLdU\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=105'>106</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dLdV\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 31'\u001b[0m in \u001b[0;36mRNN.backward\u001b[0;34m(self, x, y, clip)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=86'>87</a>\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m :\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=87'>88</a>\u001b[0m     h_t \u001b[39m=\u001b[39m h[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=88'>89</a>\u001b[0m     dLdW \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ( h_t \u001b[39m@\u001b[39;49m dht\u001b[39m.\u001b[39;49mT )\u001b[39m@np\u001b[39;49m\u001b[39m.\u001b[39;49mdiag(\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49mh[t]\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=90'>91</a>\u001b[0m xt \u001b[39m=\u001b[39m x[t][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000030?line=91'>92</a>\u001b[0m dLdU \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m xt \u001b[39m@\u001b[39m dht\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mdiag(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mh[t]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "MAX_EPOCHS = 1000\n",
    "LR=0.001\n",
    "\n",
    "model = RNN(input_dim=1, output_dim=3, hidden_dim=128)\n",
    "domain = np.linspace(-2*np.pi, 2*np.pi, 50)\n",
    "X = data_loader(domain)\n",
    "\n",
    "for epoch in tqdm(range(MAX_EPOCHS)):\n",
    "    loss = 0\n",
    "    for pair in X:\n",
    "        x,y  = pair        \n",
    "        loss += model.Loss(x, y)\n",
    "        model.step(x, y, lr=LR)\n",
    "        loss = loss / len(x)\n",
    "    print(f\"Epoch {epoch} Loss {loss}\")\n",
    "    loss_history.append(loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
