{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sympy as sy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import random\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},\\theta)  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta \\text{: parameters shared across all time steps}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, its state at time step t, is dependent only on a set a parameters and the previous state at t-1\n",
    "<br>\n",
    "<br>\n",
    "Let the state of the system, h, also be depedent on an input at the respective time step, x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},x^{t},\\theta)  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state h now contains information about the entire past history of inputs, x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a system that given the hidden state, h,produces an output o, for each time step. This output is passed to an activation function made to predict the target, y, at the respective time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "o^t= g (h^{t},\\theta')  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta' \\text{: a different set of parameters as $\\theta$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now define $\\theta$ and $\\theta'$ as the weight matrices describing the relation between the input-to-hidden, hidden-to-hidden and hidden-to-output notes; $U$, $W$ and $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z^t=  W^{T}h^{t-1} + U^{T}x^t +b \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{t} = \\phi(z^t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o^t = V^Th^{t} + c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $b$ and $c$ are biases, $\\phi$ is an activation function. <br><br>\n",
    "**Note**: matrices $U$, $W$ and $V$ are not indexed by time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each time step, we have a sequential total loss up to time step $\\tau$, $L^\\tau$, defined as the difference between our prediction and the target, at each output, upto the time step $\\tau$\n",
    "<br>\n",
    "<br>\n",
    "Consider the task of multi-class classification. \n",
    "<br>\n",
    "<br>\n",
    "Consequently, the output activation function is the normalized expontential function, a.k.a the _softmax function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\sum_{t=1}^{\\tau} l\\big(o^{t}\\big)\n",
    "\\text{: Total loss upto time step $\\tau$}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\text{: Softmax activation function for multi-class classification}\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE** the softmax is a vector function, later when taking the derivative, in reality I am finding the Jacobian of it in its vector form, but here I denote one element of it, the $i^{th}$\n",
    "\n",
    "\\begin{equation}\n",
    "l = - \\sum_{m=0}^{M-1}y_{m}^{t} \\log\\Big(\\hat{y}_{m}^{t}\\Big)\n",
    "\\text{: M categorical cross entropy for predictions at time step $t$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process differs from standard back-propagation (like descirbed for a vanilla feedforward network). Usng the above assumptions, I will go through the derivation analogous optimization process for recurrent networks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation through time\n",
    "\n",
    "Per example loss w.r.t to the output element $o_i$ at time $t$; $o_i^{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{o_{i}^{t}} L = \\frac{\\partial{L}}{\\partial{l(o_i^t)}} \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "Note that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{L}}{\\partial{l(o_i^t)}} = 1\n",
    "\\end{equation}\n",
    "and that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "is the derivative of the categorical cross-entropy\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}}\\frac{\\partial{\\hat{y}^{t}_j}}{\\partial{o_i^{t}}} } - [1]\n",
    "\\end{equation}\n",
    "The softmax functions is:\n",
    " \\begin{equation}\n",
    " \\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\end{equation}\n",
    "Taking its derivative gives:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "    \\frac{\\partial{\\hat{y}^{t}_i}}{\\partial{o_j^{t}}} = \\hat{y}^{t}_{i} \\Big( \\delta_{ij}  -  \\hat{y}^{t}_{j} \\Big)\n",
    "}- [2]\n",
    "\\end{equation}\n",
    "_look at the different cases to see why this is true_ i.e. $i=j$ and $i \\neq j$\n",
    "<br><br>\n",
    "Lets sub [2] into [1], and splitting into the cases where $i=j$ and $i \\neq j $:\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}} \\hat{y}^{t}_{j} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    " \n",
    "Lets now split the sum up for the two cases;\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j=i} + \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j \\neq i} \n",
    " \n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  -y^{t}_{i}(\\delta_{ii} - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "Simplfying down: \n",
    "\n",
    "\\begin{equation}\n",
    " \n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big( 0 -\\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} \n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}+y^{t}_{i}\\hat{y}_{i}^{t}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "Recall that $\\sum_{j} y_j = 1$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} \\Big( y_j^{t} +y^{t}_{i} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j} \\Big( y_j^{t} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets calculate the gradient on the internel nodes $h^t$ from the end of the sequence $\\tau$.\n",
    "<br>\n",
    "I am going to use vector notation here on out. I.e. $h_i^{t}$ becomes $h^t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = \\Bigg( \\frac{ \\partial{o^{\\tau}}}\n",
    "{\\partial{h^{\\tau}}} \\Bigg)^{T} \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = V \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "we iterate backwards through time. Note the dependency of $h^t$ on both $o^t$ and $h^{t+1}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = \\Bigg( \\frac{ \\partial{h^{t+1}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{h^{t+1}} L +\n",
    "\\Bigg( \\frac{ \\partial{o^{t}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{o^{t}} L \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate of the hidden units  w.r.t their previous time step is:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  \\frac{ \\partial{h^{t+1}} }{ \\partial{z^{t+1} } }\n",
    "\\frac{ \\partial{z^{t+1} } } { \\partial{h^{t}} }\n",
    "\\end{equation}\n",
    "This leads to:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  diag\\Bigg( \\phi'\\big(z^{t+1}\\big) \\Bigg) W^T\n",
    "\\end{equation}\n",
    "**Note** diag: considering only the leading diagonal values and setting all others to 0. \n",
    "<br><br>\n",
    "For RNNs , we want to use a saturating activation to avoid gradient explosions <br><br>\n",
    "e.g. hyperbolic tagent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \\phi'\\big(z^{t+1}\\big) \\Big)   \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets specify the activation function (using the hyperpolic tagent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \n",
    "     1 - \\big(h^{t+1}\\big)^2\n",
    "    \\Big)  \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradients on the biases $b$ and $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\Bigg(\n",
    "     \\frac{\\partial{o^t}}{\\partial{c^t}} \n",
    "     \\Bigg)^{T} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "since $\\frac{\\partial{o^t}}{\\partial{c^t}} = 1$\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  \\Bigg(\n",
    "     \\frac{\\partial{h^t}}{\\partial{b^t}} \n",
    "     \\Bigg)^{T}  \\nabla_{h^t} L\n",
    "\\end{equation}\n",
    "Since $b$ is dependent on h through the activation function $\\phi$, we have: \n",
    "\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  diag \\Bigg( \\phi' \\Big( z^t \\Big) \\Bigg) \\nabla_{h^t} L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative w.r.t to $V$; the hidden-ouput matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{V} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial o_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{V} O_i^{t}\n",
    "\\end{equation}\n",
    "Leading to:\n",
    "\\begin{equation}\n",
    "\\nabla_{V} L  = \\sum_{t} h^t \\Big(\\nabla_{o^t} L \\Big)^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative w.r.t the weight matrices $W$ and $U$, we introduce dummy variables $W^t$ and $U^t$. These are copies of each other at each time step, summing these up will give us the total gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{W} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{W^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\nabla_{W} L  = \\sum_{t} h^{t-1} \\Big(\\nabla_{h^t} L \\Big)^T  diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\\end{equation}\n",
    "for the derivative of w.r.t $U$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{U} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{U^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\nabla_{U} L  = \\sum_{t} x^{t} \\Big( \\nabla_{h^t} L \\Big)^T \n",
    "     diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling RNN with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        # network variables \n",
    "        self.idim = input_dim\n",
    "        self.hdim = hidden_dim\n",
    "        # initialise weights \n",
    "        self.U = np.random.uniform(- np.sqrt(1./self.idim),\n",
    "                                     np.sqrt(1./self.idim),\n",
    "                                    (self.idim, self.hdim) )\n",
    "\n",
    "        self.V = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.idim))\n",
    "\n",
    "        self.W = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.hdim))\n",
    "\n",
    "        self.b = np.zeros(self.hdim)\n",
    "        self.c = np.zeros(self.idim)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        '''Note that this is a numerically stable version of softmax.\n",
    "        \n",
    "        We substract the max value from all elements.\n",
    "        Overflow of a single element, or underflow of all elements,  will render the output usless.\n",
    "        \n",
    "        subtracting max leaves only non-positive values ---> no overflow \n",
    "        at least one element = 0 ---> no vanishing denominator (underflow is some enteries is okay) \n",
    "         '''\n",
    "        xt = np.exp(x-np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Single example pass forward, all the way through the network\n",
    "        T = len(x)\n",
    "\n",
    "        # will stack as rows\n",
    "        h = np.zeros((T,self.hdim))\n",
    "        o = np.zeros((T,self.idim))\n",
    "\n",
    "        for t in range(T):\n",
    "            #### indexing by x[t], where x[t] is a one hot encoded vector, is the same as multiplying by it.  \n",
    "            h[t] = self.U[x[t], :] + self.b\n",
    "            \n",
    "            # add contribution from previous time step\n",
    "            if t > 1:\n",
    "                h[t] += self.W @ h[t-1] + self.c\n",
    "\n",
    "            h[t] = np.tanh(h[t])\n",
    "            o[t] = self.softmax( self.V @ h[t] + self.c)\n",
    "\n",
    "        return (o,h)\n",
    "\n",
    "    def backward(self, x, y, clip=None):\n",
    "        T = len(x)\n",
    "        o,h = self.forward(x)\n",
    "\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdb = np.zeros(self.b.shape)\n",
    "        dLdc = np.zeros(self.c.shape)\n",
    "\n",
    "        # dL/do\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # dL/dh\n",
    "        delta_h = np.zeros((T, self.hdim))\n",
    "       \n",
    "        for t in reversed(range(T)):\n",
    "\n",
    "            # collect errors on hidden states\n",
    "            delta_h[t] = self.V @ delta_o[t,:]\n",
    "            if t < T-1:\n",
    "                # collect errors on hidden states due to W\n",
    "                delta_h[t] = ( self.W @ np.diag(1-h[t+1]**2) ) @ delta_h[t+1]\n",
    "\n",
    "\n",
    "        for t in range(T):\n",
    "            # error on ouput bias\n",
    "            dLdc += delta_o[t,:]\n",
    "            # error on hidden bias \n",
    "            dLdb += (1-h[t]**2) * delta_h[t,:]\n",
    "            # error on hidden-output matrix\n",
    "            dLdV += h[t,:]@ delta_o[t,:].T \n",
    "            \n",
    "            # error on hidden-hidden W\n",
    "            if t > 0 :\n",
    "                dLdW += ( h[t-1,:] @ delta_h[t,:].T )@np.diag(1-h[t]**2)\n",
    "            \n",
    "            # 1 hot encoded vector \n",
    "            xm = np.zeros((self.idim))\n",
    "            xm[x] = 1. \n",
    "\n",
    "            dLdU += xm @ delta_h[t,:].T @ np.diag(1-h[t]**2)\n",
    "\n",
    "        if clip is not None:\n",
    "            dLdb = np.clip(dLdb, -clip, clip)\n",
    "            dLdc = np.clip(dLdc, -clip, clip)\n",
    "            dLdV = np.clip(dLdV, -clip, clip)\n",
    "            dLdW = np.clip(dLdW, -clip, clip)\n",
    "            dLdU = np.clip(dLdU, -clip, clip)\n",
    "\n",
    "        return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
    "\n",
    "\n",
    "\n",
    "    def step(self,x,y,lr=0.01):\n",
    "        dLdU, dLdV, dLdW, dLdb, dLdc = self.backward(x,y)\n",
    "        self.U -= lr * dLdU\n",
    "        self.V -= lr * dLdV\n",
    "        self.W -= lr * dLdW \n",
    "        self.b -= lr * dLdb \n",
    "        self.c -= lr * dLdc \n",
    "    \n",
    "\n",
    "    def Loss(self, x,y):\n",
    "        o,h = self.forward(x)\n",
    "        return -np.sum(o[np.arange(len(y)), y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Bird sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import glob\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob \n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing as mp  \n",
    "import pydub \n",
    "# InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths():\n",
    "    files = glob.glob(\"/home/akinwilson/Projects/bird-sound-classifier/data/birds/*/*/*\")\n",
    "    files = [f for f in files if f.endswith(\".mp3\")]\n",
    "    return files\n",
    "def test_load_audio(f):\n",
    "    try:\n",
    "        pydub.AudioSegment.from_mp3(f)\n",
    "        return f\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m./data/total.p\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "File \u001b[0;32m~/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=629'>630</a>\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=630'>631</a>\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=631'>632</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=633'>634</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=634'>635</a>\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=635'>636</a>\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=636'>637</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/frame.py?line=637'>638</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=493'>494</a>\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=494'>495</a>\u001b[0m         x\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=495'>496</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=496'>497</a>\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=497'>498</a>\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=498'>499</a>\u001b[0m     ]\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=499'>500</a>\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=501'>502</a>\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=116'>117</a>\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=117'>118</a>\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=118'>119</a>\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=119'>120</a>\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=120'>121</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=121'>122</a>\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=660'>661</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=662'>663</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=663'>664</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=665'>666</a>\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[1;32m    <a href='file:///home/akinwilson/.virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=666'>667</a>\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "def get_label_and_path(file_path):\n",
    "    parts = tf.strings.split(input=file_path,sep=os.path.sep)\n",
    "    return (file_path,parts[-3].numpy().decode(\"utf-8\"))\n",
    "\n",
    "def create_clean_ds():\n",
    "    files = get_file_paths()\n",
    "    pool = mp.Pool(mp.cpu_count()-2)\n",
    "    files_clean = pool.map(test_load_audio,files )\n",
    "    files_clean = [f for f in files_clean if f is not None]\n",
    "    x = dict([get_label_and_path(f) for f in files_clean])\n",
    "    pickle.dump(file=open(\"./data/total.p\",\"wb\"), obj=x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    df = pd.DataFrame.from_dict(pickle.load(open(f\"./data/{name}.p\", \"rb\")).items())\n",
    "    return df \n",
    "\n",
    "\n",
    "def create_train_test_split(df):\n",
    "    return train_test_split( df[0], df[1], test_size=0.33, random_state=42)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest =  create_train_test_split(get_dataset(\"total\"))\n",
    "\n",
    "def save_train_test(data_dict, name):\n",
    "    pickle.dump(data_dict, open(f\"./data/{name}\", \"wb\"))\n",
    "\n",
    "# train = dict(zip(Xtrain, Ytrain))\n",
    "# test = dict(zip(Xtest, Ytest))\n",
    "# save_train_test(train, \"train.p\")\n",
    "# save_train_test(test,\"test.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import math\n",
    "import scipy.io as sio\n",
    "\n",
    "loadable_files = []\n",
    "for f in files:\n",
    "    try:\n",
    "        pydub.AudioSegment.from_mp3(f)\n",
    "        loadable_files.append(f)\n",
    "    except:\n",
    "        print(f\"Except: {f}\")\n",
    "\n",
    "# audio.rate * 30 \n",
    "# x = trim(stero2mono(audio))\n",
    "\n",
    "def repeat_or_trim_audio(file_path):\n",
    "    audio =  get_audio(file_path)\n",
    "    x = trim(stero2mono(audio))\n",
    "\n",
    "    SAMPLE_LEN = audio.rate  * 30  \n",
    "    to_repeat = np.ceil(( SAMPLE_LEN / x.shape[0] ).numpy())\n",
    "    \n",
    "    if to_repeat == 1.0:\n",
    "        print(\"trimmed\")\n",
    "        return x[:SAMPLE_LEN]\n",
    "    elif not math.isinf(to_repeat) and to_repeat > 1.0:\n",
    "\n",
    "        print(\"extended \")\n",
    "        print(\"Keep upto: \", SAMPLE_LEN, \"len\")\n",
    "        print(\"to_repeat:\", to_repeat)\n",
    "        print(\"current dim\", x.shape)\n",
    "        print(\"repeated len\", tf.repeat(x, to_repeat, axis=0).shape)\n",
    "        return tf.repeat(x, to_repeat, axis=0)[:SAMPLE_LEN]\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        loss += model.loss(X_train[i], Y_train[i])\n",
    "    loss = loss / len(X_train)\n",
    "    print(\"Epoch {0} Loss {1}\".format(epoch , loss))\n",
    "    loss_history.append(loss)\n",
    "    for i in range(len(X_train)):\n",
    "       model.step(X_train[i], Y_train[i], learning_rate=LR)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
