{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "import sympy as sy\n",
    "from tqdm.notebook import tqdm\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},\\theta)  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta \\text{: parameters shared across all time steps}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, its state at time step t, is dependent only on a set a parameters and the previous state at t-1\n",
    "<br>\n",
    "<br>\n",
    "Let the state of the system, h, also be depedent on an input at the respective time step, x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h^t= f (h^{t-1},x^{t},\\theta)  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state h now contains information about the entire past history of inputs, x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now a system that given the hidden state, h,produces an output o, for each time step. This output is passed to an activation function made to predict the target, y, at the respective time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "o^t= g (h^{t},\\theta')  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta' \\text{: a different set of parameters as $\\theta$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now define $\\theta$ and $\\theta'$ as the weight matrices describing the relation between the input-to-hidden, hidden-to-hidden and hidden-to-output notes; $U$, $W$ and $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "z^t=  W^{T}h^{t-1} + U^{T}x^t +b \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{t} = \\phi(z^t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o^t = V^Th^{t} + c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $b$ and $c$ are biases, $\\phi$ is an activation function. <br><br>\n",
    "**Note**: matrices $U$, $W$ and $V$ are not indexed by time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each time step, we have a sequential total loss up to time step $\\tau$, $L^\\tau$, defined as the difference between our prediction and the target, at each output, upto the time step $\\tau$\n",
    "<br>\n",
    "<br>\n",
    "Consider the task of multi-class classification. \n",
    "<br>\n",
    "<br>\n",
    "Consequently, the output activation function is the normalized expontential function, a.k.a the _softmax function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L = \\sum_{t=1}^{\\tau} l\\big(o^{t}\\big)\n",
    "\\text{: Total loss upto time step $\\tau$}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\text{: Softmax activation function for multi-class classification}\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE** the softmax is a vector function, later when taking the derivative, in reality I am finding the Jacobian of it in its vector form, but here I denote one element of it, the $i^{th}$\n",
    "\n",
    "\\begin{equation}\n",
    "l = - \\sum_{m=0}^{M-1}y_{m}^{t} \\log\\Big(\\hat{y}_{m}^{t}\\Big)\n",
    "\\text{: M categorical cross entropy for predictions at time step $t$}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process differs from standard back-propagation (like descirbed for a vanilla feedforward network). Usng the above assumptions, I will go through the derivation analogous optimization process for recurrent networks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation through time\n",
    "\n",
    "Per example loss w.r.t to the output element $o_i$ at time $t$; $o_i^{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{o_{i}^{t}} L = \\frac{\\partial{L}}{\\partial{l(o_i^t)}} \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "Note that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{L}}{\\partial{l(o_i^t)}} = 1\n",
    "\\end{equation}\n",
    "and that:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}}\n",
    "\\end{equation}\n",
    "is the derivative of the categorical cross-entropy\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}}\\frac{\\partial{\\hat{y}^{t}_j}}{\\partial{o_i^{t}}} } - [1]\n",
    "\\end{equation}\n",
    "The softmax functions is:\n",
    " \\begin{equation}\n",
    " \\hat{y}^t_i = \\frac{\\exp(o_i^t)}{\\sum_{j}\\exp(o_j^t)}\n",
    "\\end{equation}\n",
    "Taking its derivative gives:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "    \\frac{\\partial{\\hat{y}^{t}_i}}{\\partial{o_j^{t}}} = \\hat{y}^{t}_{i} \\Big( \\delta_{ij}  -  \\hat{y}^{t}_{j} \\Big)\n",
    "}- [2]\n",
    "\\end{equation}\n",
    "_look at the different cases to see why this is true_ i.e. $i=j$ and $i \\neq j$\n",
    "<br><br>\n",
    "Lets sub [2] into [1], and splitting into the cases where $i=j$ and $i \\neq j $:\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} \\frac{y_j^{t}}{\\hat{y}_j^{t}} \\hat{y}^{t}_{j} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    "\n",
    " \\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = - \\sum_{j} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation}\n",
    " \n",
    "Lets now split the sum up for the two cases;\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j=i} + \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} \\Bigr|_{j \\neq i}  =  -y^{t}_{i}(\\delta_{ii} - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big(\\delta_{ij}  -  \\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "Simplfying down: \n",
    "\n",
    "\\begin{equation}\n",
    " \n",
    " \\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} - \\sum_{j \\neq i} y_j^{t} \\Big( 0 -\\hat{y}^{t}_{i} \\Big)\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}  -y^{t}_{i}(1 - \\hat{y}_{i})^{t} \n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} y_j^{t} \\hat{y}^{t}_{i}+y^{t}_{i}\\hat{y}_{i}^{t}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "Recall that $\\sum_{j} y_j = 1$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j \\neq i} \\Big( y_j^{t} +y^{t}_{i} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} = \\sum_{j} \\Big( y_j^{t} \\Big) \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\frac{\\partial{l(o_i^t)}}{\\partial o_{i}^{t}} =  \\hat{y}^{t}_{i}  -y^{t}_{i}\n",
    "}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets calculate the gradient on the internel nodes $h^t$ from the end of the sequence $\\tau$.\n",
    "<br>\n",
    "I am going to use vector notation here on out. I.e. $h_i^{t}$ becomes $h^t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = \\Bigg( \\frac{ \\partial{o^{\\tau}}}\n",
    "{\\partial{h^{\\tau}}} \\Bigg)^{T} \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^\\tau} L = V \\nabla_{o^\\tau} L\n",
    "\\end{equation}\n",
    "we iterate backwards through time. Note the dependency of $h^t$ on both $o^t$ and $h^{t+1}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = \\Bigg( \\frac{ \\partial{h^{t+1}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{h^{t+1}} L +\n",
    "\\Bigg( \\frac{ \\partial{o^{t}}}\n",
    "{\\partial{h^{t}}} \\Bigg)^{T} \\nabla_{o^{t}} L \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate of the hidden units  w.r.t their previous time step is:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  \\frac{ \\partial{h^{t+1}} }{ \\partial{z^{t+1} } }\n",
    "\\frac{ \\partial{z^{t+1} } } { \\partial{h^{t}} }\n",
    "\\end{equation}\n",
    "This leads to:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{ \\partial{h^{t+1}} }\n",
    "{\\partial{h^{t}} }  =  diag\\Bigg( \\phi'\\big(z^{t+1}\\big) \\Bigg) W^T\n",
    "\\end{equation}\n",
    "**Note** diag: considering only the leading diagonal values and setting all others to 0. \n",
    "<br><br>\n",
    "For RNNs , we want to use a saturating activation to avoid gradient explosions <br><br>\n",
    "e.g. hyperbolic tagent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \\phi'\\big(z^{t+1}\\big) \\Big)   \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets specify the activation function (using the hyperpolic tagent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{h^t} L = W  diag \\Big( \n",
    "     1 - \\big(h^{t+1}\\big)^2\n",
    "    \\Big)  \\nabla_{h^{t+1}} L +\n",
    "V \\nabla_{o^{t}} L \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradients on the biases $b$ and $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\Bigg(\n",
    "     \\frac{\\partial{o^t}}{\\partial{c^t}} \n",
    "     \\Bigg)^{T} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "since $\\frac{\\partial{o^t}}{\\partial{c^t}} = 1$\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{c} L  = \\sum_{t} \\nabla_{o^t} L\n",
    "\\end{equation}\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  \\Bigg(\n",
    "     \\frac{\\partial{h^t}}{\\partial{b^t}} \n",
    "     \\Bigg)^{T}  \\nabla_{h^t} L\n",
    "\\end{equation}\n",
    "Since $b$ is dependent on h through the activation function $\\phi$, we have: \n",
    "\n",
    "Next:\n",
    "\\begin{equation}\n",
    "\\nabla_{b} L  = \\sum_{t}  diag \\Bigg( \\phi' \\Big( z^t \\Big) \\Bigg) \\nabla_{h^t} L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative w.r.t to $V$; the hidden-ouput matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{V} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial o_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{V} O_i^{t}\n",
    "\\end{equation}\n",
    "Leading to:\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{V} L  = \\sum_{t} h^t \\Big(\\nabla_{o^t} L \\Big)^T\n",
    "}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative w.r.t the weight matrices $W$ and $U$, we introduce dummy variables $W^t$ and $U^t$. These are copies of each other at each time step, summing these up will give us the total gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla_{W} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{W^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{W} L  = \\sum_{t} h^{t-1} \\Big(\\nabla_{h^t} L \\Big)^T  diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n",
    "for the derivative of w.r.t $U$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{U} L  = \\sum_{t} \\sum_{i}  \\Bigg(\n",
    "    \\frac{ \\partial L}{ \\partial h_{i}^t}\n",
    "     \\Bigg)^T \\nabla_{U^t} h_i^{t}\n",
    "\\end{equation}\n",
    "giving: \n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\nabla_{U} L  = \\sum_{t} x^{t} \\Big( \\nabla_{h^t} L \\Big)^T \n",
    "     diag \\Bigg( \\phi ' \\big(z^t \\big) \\Bigg)\n",
    "\n",
    "}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling RNN with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        # network variables \n",
    "        self.idim = input_dim\n",
    "        self.hdim = hidden_dim\n",
    "        self.odim = output_dim\n",
    "        # initialise weights \n",
    "        self.U = np.random.uniform(- np.sqrt(1./self.idim),\n",
    "                                     np.sqrt(1./self.idim),\n",
    "                                    (self.idim, self.hdim) )\n",
    "\n",
    "        self.V = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.odim))\n",
    "\n",
    "        self.W = np.random.uniform( -np.sqrt(1./self.hdim),\n",
    "                                     np.sqrt(1./self.hdim), \n",
    "                                    (self.hdim,self.hdim))\n",
    "\n",
    "        self.b = np.zeros(self.hdim)\n",
    "        self.c = np.zeros(self.odim)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        '''Note that this is a numerically stable version of softmax.\n",
    "        \n",
    "        We substract the max value from all elements.\n",
    "        Overflow of a single element, or underflow of all elements,  will render the output usless.\n",
    "        \n",
    "        subtracting max leaves only non-positive values ---> no overflow \n",
    "        at least one element = 0 ---> no vanishing denominator (underflow is some enteries is okay) \n",
    "         '''\n",
    "        xt = np.exp(x-np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Single example pass forward, all the way through the network\n",
    "        T = len(x)\n",
    "        # will stack as rows\n",
    "        h = np.zeros((T,self.hdim))\n",
    "        o = np.zeros((T,self.odim))\n",
    "        for t in range(T):\n",
    "            # print(\"x[t], inside forward\", x[t])\n",
    "            h[t] = self.U.T @ x[t] + self.b\n",
    "            # print(\"h[t], inside forward\", h[t])\n",
    "            # add contribution from previous time step\n",
    "            if t > 1:\n",
    "                h[t] += self.W @ h[t-1] + self.b\n",
    "            h[t] = np.tanh(h[t])\n",
    "            # print(\"foward softmax output\", self.softmax( self.V.T @ h[t] + self.c))\n",
    "            o[t] = self.softmax( self.V.T @ h[t] + self.c)\n",
    "\n",
    "        return (o,h)\n",
    "\n",
    "    def backward(self, x, y, clip=None):\n",
    "        T = len(x)\n",
    "        o,h = self.forward(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdb = np.zeros(self.b.shape)\n",
    "        dLdc = np.zeros(self.c.shape)\n",
    "        # dL/do\n",
    "        delta_o = o\n",
    "        delta_o[ np.arange(len(y)), y ] -= 1.\n",
    "        # dL/dh\n",
    "        delta_h = np.zeros((T, self.hdim))\n",
    "       \n",
    "        for t in reversed(range(T)):\n",
    "            # collect errors on hidden states\n",
    "            delta_h[t] = self.V @ delta_o[t,:]\n",
    "            if t < T-1:\n",
    "                # collect errors on hidden states due to W\n",
    "                delta_h[t] = ( self.W @ np.diag(1-h[t+1]**2) ) @ delta_h[t+1]\n",
    "\n",
    "\n",
    "        for t in range(T):\n",
    "            # error on ouput bias\n",
    "            dLdc += delta_o[t,:]\n",
    "            # error on hidden bias \n",
    "            dLdb += (1-h[t]**2) * delta_h[t,:]\n",
    "            # error on hidden-output matrix\n",
    "            \n",
    "            ot = delta_o[t,:][...,np.newaxis]\n",
    "            ht = h[t,:][...,np.newaxis]            \n",
    "            dht = delta_h[t,:][...,np.newaxis]\n",
    "                \n",
    "\n",
    "            dLdV += ht @ ot.T \n",
    "            # error on hidden-hidden W\n",
    "            if t > 0 :\n",
    "                h_t = h[t-1,:][...,np.newaxis]\n",
    "                dLdW += ( h_t @ dht.T )@np.diag(1-h[t]**2)\n",
    "\n",
    "            xt = x[t][...,np.newaxis]\n",
    "            dLdU += xt @ dht.T @ np.diag(1-h[t]**2)\n",
    "\n",
    "        if clip is not None:\n",
    "            dLdb = np.clip(dLdb, -clip, clip)\n",
    "            dLdc = np.clip(dLdc, -clip, clip)\n",
    "            dLdV = np.clip(dLdV, -clip, clip)\n",
    "            dLdW = np.clip(dLdW, -clip, clip)\n",
    "            dLdU = np.clip(dLdU, -clip, clip)\n",
    "\n",
    "        return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
    "\n",
    "\n",
    "\n",
    "    def step(self,x,y,lr=0.01):\n",
    "        dLdU, dLdV, dLdW, dLdb, dLdc = self.backward(x,y)\n",
    "        self.U -= lr * dLdU\n",
    "        self.V -= lr * dLdV\n",
    "        self.W -= lr * dLdW \n",
    "        self.b -= lr * dLdb \n",
    "        self.c -= lr * dLdc \n",
    "    \n",
    "\n",
    "    def Loss(self, x,y):\n",
    "        o,h = self.forward(x)\n",
    "        # print(\"output layer output:\", o.shape)\n",
    "        # print(\"Indexed at y \", o[np.arange(len(y)), y].shape)\n",
    "        \n",
    "        LOSS = -np.sum(o[np.arange(len(y)), y])\n",
    "        # print(\"Loss indexed using arange and y: \", LOSS)\n",
    "        return LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling synthetic time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000044?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mpi, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mpi, \u001b[39m50\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000044?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnoisy_sin_wave\u001b[39m(x):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000044?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msin(x) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(loc\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, scale\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.linspace(-2*np.pi, 2*np.pi, 50)\n",
    "\n",
    "def noisy_sin_wave(x):\n",
    "    return np.sin(x) + np.random.normal(loc=0, scale=0.5)\n",
    "\n",
    "def noisy_cos_wave(x):\n",
    "    return np.cos(x) + np.random.normal(loc=0, scale=0.5)\n",
    "\n",
    "def plot_noisy_func(func, domain,title):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.set_title(title)\n",
    "    ax.plot(domain,np.vectorize(func)(domain), \".\")\n",
    "\n",
    "\n",
    "def create_sample_seq(func1,func2, domain, label):\n",
    "    X = np.array( [np.vectorize(func1)(domain),\n",
    "                    np.vectorize(func2)(domain)])\n",
    "    Y = np.array([[label]*X.shape[1]])\n",
    "    return X,Y\n",
    "\n",
    "plot_noisy_func(noisy_sin_wave, X, \"Noisy sin wave\")\n",
    "plot_noisy_func(noisy_cos_wave, X, \"Noisy cos wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}sin_{e}\\\\sin_{e'}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ sin_e],\n",
       "[sin_e']])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_sin,var_cos, var_sinp,var_cosp = sy.symbols(\"sin_e,cos_e, sin_e', cos_e'\")\n",
    "sy.Matrix([ [var_sin], [var_sinp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}cos_{e}\\\\cos_{e'}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ cos_e],\n",
       "[cos_e']])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix([ [var_cos], [var_cosp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}sin_{e}\\\\cos_{e}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[sin_e],\n",
       "[cos_e]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.Matrix( [[ var_sin], [var_cos]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_sample_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=3'>4</a>\u001b[0m     X2 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_sin_wave,noisy_cos_wave, X, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39m*\u001b[39mX0, \u001b[39m*\u001b[39mX1, \u001b[39m*\u001b[39mX2]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m data_loader(sample_size\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 38'\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(sample_size)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_loader\u001b[39m(sample_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=1'>2</a>\u001b[0m     X0 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_sin_wave,noisy_sin_wave, X, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=2'>3</a>\u001b[0m     X1 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_cos_wave,noisy_cos_wave, X, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=3'>4</a>\u001b[0m     X2 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_sin_wave,noisy_cos_wave, X, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 38'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_loader\u001b[39m(sample_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=1'>2</a>\u001b[0m     X0 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_sin_wave,noisy_sin_wave, X, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=2'>3</a>\u001b[0m     X1 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_cos_wave,noisy_cos_wave, X, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000047?line=3'>4</a>\u001b[0m     X2 \u001b[39m=\u001b[39m [ create_sample_seq(noisy_sin_wave,noisy_cos_wave, X, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_sample_seq' is not defined"
     ]
    }
   ],
   "source": [
    "def data_loader(sample_size=30):\n",
    "    X0 = [ create_sample_seq(noisy_sin_wave,noisy_sin_wave, X, 0) for _ in range(sample_size)]\n",
    "    X1 = [ create_sample_seq(noisy_cos_wave,noisy_cos_wave, X, 1) for _ in range(sample_size)]\n",
    "    X2 = [ create_sample_seq(noisy_sin_wave,noisy_cos_wave, X, 2) for _ in range(sample_size)]\n",
    "    return [*X0, *X1, *X2]\n",
    "\n",
    "x = data_loader(sample_size=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting echo: 0\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eb8f48c0c24d43b9fbadb5955d2894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss -1.8792177159433945e-06\n",
      "Starting echo: 1\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca74b68bf5004712b2544505bc0f464f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss -1.3519733119506076e-24\n",
      "Starting echo: 2\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a064071ebe394d5d90af3859e090c63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss -4.6694100830015505e-31\n",
      "Starting echo: 3\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03ddc1a40914d94b3b7568e7ac17c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss -1.5333308736112068e-34\n",
      "Starting echo: 4\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d82d1d071c40d2a97fa64ea128e95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss -1.9387722077641305e-34\n",
      "Starting echo: 5\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e2c0c2b8c24a228796cbf93a4c738a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss -1.9128101524606286e-31\n",
      "Starting echo: 6\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0d324b0b9947baa78e6402af9080d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss -1.3021111562645196e-34\n",
      "Starting echo: 7\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce7d2905c054aa09b435c028ffc6d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss -4.609982767405766e-33\n",
      "Starting echo: 8\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1221bd0856ae4a0cb3bf5d8de4d62d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss -9.895605273336343e-32\n",
      "Starting echo: 9\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f21e3a1bc014cb38c1e8340bb2fd179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss -8.723371783590374e-33\n",
      "Starting echo: 10\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17bac04b32b486d85e69e7e1d25da78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss -5.486459801273051e-35\n",
      "Starting echo: 11\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac01515560914ccc918d0c8c21edf33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss -5.190961685651037e-35\n",
      "Starting echo: 12\n",
      "number of sequence pairs: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1095f74ac174cc9b85043f79722623a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# print(f\"Sequence length: {len(y)}\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mLoss(x, y)\n\u001b[0;32m---> 17\u001b[0m     model\u001b[39m.\u001b[39;49mstep(x, y, lr\u001b[39m=\u001b[39;49mLR)\n\u001b[1;32m     18\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 35'\u001b[0m in \u001b[0;36mRNN.step\u001b[0;34m(self, x, y, lr)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m,x,y,lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=109'>110</a>\u001b[0m     dLdU, dLdV, dLdW, dLdb, dLdc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(x,y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=110'>111</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dLdU\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=111'>112</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dLdV\n",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb Cell 35'\u001b[0m in \u001b[0;36mRNN.backward\u001b[0;34m(self, x, y, clip)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=92'>93</a>\u001b[0m         dLdW \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ( h_t \u001b[39m@\u001b[39m dht\u001b[39m.\u001b[39mT )\u001b[39m@np\u001b[39m\u001b[39m.\u001b[39mdiag(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mh[t]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=94'>95</a>\u001b[0m     xt \u001b[39m=\u001b[39m x[t][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=95'>96</a>\u001b[0m     dLdU \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m xt \u001b[39m@\u001b[39;49m dht\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m np\u001b[39m.\u001b[39;49mdiag(\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49mh[t]\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m clip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/6_from_plane_to_sequential.ipynb#ch0000034?line=98'>99</a>\u001b[0m     dLdb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(dLdb, \u001b[39m-\u001b[39mclip, clip)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "MAX_EPOCHS = 100\n",
    "LR=0.01\n",
    "\n",
    "model = RNN(input_dim=2, output_dim=3, hidden_dim=64)\n",
    "X = data_loader(dev_run=True)\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(f\"Starting echo: {epoch}\")\n",
    "    loss = 0\n",
    "    print(f\"number of sequence pairs: {len(X)}\")\n",
    "    for pair in tqdm(X):\n",
    "        x,y  = pair\n",
    "        loss += model.Loss(x, y)\n",
    "        model.step(x, y, lr=LR)\n",
    "        loss = loss / len(x)\n",
    "    print(f\"Epoch {epoch} Loss {loss}\")\n",
    "    loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    df = pd.DataFrame.from_dict(pickle.load(open(f\"./data/{name}.p\", \"rb\")).items())\n",
    "    return df \n",
    "\n",
    "\n",
    "def create_clean_ds():\n",
    "    files = get_file_paths()\n",
    "    pool = mp.Pool(mp.cpu_count()-2)\n",
    "    files_clean = pool.map(test_load_audio,files )\n",
    "    files_clean = [f for f in files_clean if f is not None]\n",
    "    x = dict([get_label_and_path(f) for f in files_clean])\n",
    "    pickle.dump(file=open(\"./data/total.p\",\"wb\"), obj=x)\n",
    "    \n",
    "\n",
    "def create_train_test_split(df):\n",
    "    return train_test_split( df[0], df[1], test_size=0.33, random_state=42)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest =  create_train_test_split(get_dataset(\"total\"))\n",
    "\n",
    "def save_train_test(data_dict, name):\n",
    "    pickle.dump(data_dict, open(f\"./data/{name}\", \"wb\"))\n",
    "\n",
    "# train = dict(zip(Xtrain, Ytrain))\n",
    "# test = dict(zip(Xtest, Ytest))\n",
    "# save_train_test(train, \"train.p\")\n",
    "# save_train_test(test,\"test.p\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
