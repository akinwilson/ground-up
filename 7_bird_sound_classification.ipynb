{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "import pickle\n",
    "import numpy as np\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch                                      # root package\n",
    "from torch.utils.data import Dataset, DataLoader  # data \n",
    "import torch.nn as nn                             # neural networks\n",
    "import torch.nn.functional as F                   # layers, activations and more\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio                                 # audio processing\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    df = pd.DataFrame.from_dict(pickle.load(open(f\"./data/{name}.p\", \"rb\")).items())\n",
    "    df.rename(columns={0:\"path\", 1:\"class\"}, inplace=True)\n",
    "    return df\n",
    "def get_df(name):\n",
    "    return pickle.load(open(f\"./data/{name}.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_ds():\n",
    "    df = get_dataset(\"total\")\n",
    "    train, validate, test = np.split(df.sample(frac=1, random_state=42), [int(.6*len(df)), int(.8*len(df))])\n",
    "    dfs = {\"train\":train,\"validate\":validate, \"test\":test}\n",
    "    for name,df_split in dfs.items():\n",
    "        print(\"size:\", df_split.shape)\n",
    "        print(df_split.columns)\n",
    "        pickle.dump(df_split, open(f\"./data/{name}.p\", \"wb\"))\n",
    "        df_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"*\"*10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"*\"*10)\n",
    "    print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "    print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "    print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "    print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Amplitude\")\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "        \n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "    num_channels, _ = waveform.shape\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate, sides=\"onesided\")\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Frequency [Hz]\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_MP3_PATH = (\"/home/akinwilson/Projects/bird-sound-classifier/data/birds/Phylloscopuscollybita/Poland\"\n",
    "\"/Phylloscopuscollybita325319.mp3\")\n",
    "metadata = torchaudio.info(SAMPLE_MP3_PATH)\n",
    "print(\"metadata: \", metadata)\n",
    "def _get_sample(path, resample=None):\n",
    "    effects = [[\"remix\", \"1\"]]\n",
    "    if resample:\n",
    "        effects.extend(\n",
    "            [\n",
    "                [\"lowpass\", f\"{resample // 2}\"],\n",
    "                [\"rate\", f\"{resample}\"],\n",
    "            ]\n",
    "        )\n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_sample(SAMPLE_WAV_PATH, resample=None):\n",
    "    return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "SAMPLE_RATE = 32000\n",
    "waveform, sample_rate = get_sample(SAMPLE_MP3_PATH, resample=SAMPLE_RATE)\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "plot_specgram(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram = torchaudio.transforms.Spectrogram()\n",
    "normalise_img = torchvision.transforms.Normalize((0.485,), (0.225,))\n",
    "transformations = torch.nn.Sequential(extract_spectrogram,\n",
    "                                      normalise_img)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,ds_name, input_transform=None):\n",
    "        sample_size = 10 if ds_name == \"validate\" else 200\n",
    "        self.files = get_df(f\"{ds_name}\").sample(n=sample_size)  # .samepl[:20]\n",
    "        self.cls2id = {name:idx for idx,name in enumerate(list(self.files['class'].unique()))}\n",
    "        self.input_transform = input_transform\n",
    "        self.NEW_SAMPLE_RATE = 16000\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.files.iloc[idx]['path']\n",
    "        data_label = self.files.iloc[idx]['class']\n",
    "        audio_data, OLD_SAMPLE_RATE = torchaudio.load(data_path, normalize=True)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:OLD_SAMPLE_RATE * 10], 0)\n",
    "        apply_resampling  = torchaudio.transforms.Resample(orig_freq= OLD_SAMPLE_RATE ,\n",
    "                                                           new_freq= self.NEW_SAMPLE_RATE,\n",
    "                                                           resampling_method= 'sinc_interpolation',\n",
    "                                                           lowpass_filter_width = 6,\n",
    "                                                           rolloff = 0.99)\n",
    "        audio_data = apply_resampling(audio_data)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE*10], 0)\n",
    "\n",
    "        while audio_data.shape[-1] < self.NEW_SAMPLE_RATE*10:\n",
    "            audio_data = torch.concat((audio_data,audio_data), dim=1)\n",
    "            audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE * 10], 0)\n",
    "            \n",
    "        if self.input_transform:\n",
    "            audio_input = self.input_transform(audio_data)\n",
    "        \n",
    "        target_label = torch.tensor(self.cls2id[data_label], dtype=torch.int16).type(torch.LongTensor)        \n",
    "        return audio_input, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=6,  kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,  out_channels=16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=6,  kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=6,  out_channels=1,  kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop_out = torch.nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(480, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used for training: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "net = Net(num_classes=19)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss( )\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AudioDataset(ds_name=\"train\", input_transform=transformations)\n",
    "ds_val = AudioDataset(ds_name=\"validate\", input_transform=transformations)\n",
    "\n",
    "train_dataloader = DataLoader(ds_train, num_workers=4, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(ds_val, num_workers=4, batch_size=128, shuffle=True)\n",
    "\n",
    "MAX_EPOCHS = 100\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "accuracy_hist = []\n",
    "\n",
    "for epoch in tqdm(range(MAX_EPOCHS)):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(\"loss: \", loss.item())\n",
    "    print(\"*\"*40)\n",
    "    print(f'Epoch:{epoch + 1}\\nAvg loss: {running_loss /len(train_dataloader)}')\n",
    "    print(\"*\"*40)\n",
    "    train_loss_hist.append(loss.item())\n",
    "    print(\"Entering validation...\")\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            inputs, true_labels = data\n",
    "            true = true_labels.numpy() \n",
    "            inputs = inputs.to(device)\n",
    "            pred_labels = net(inputs)\n",
    "            predicted = [(pred_label == torch.max(pred_label)).nonzero().flatten().cpu().numpy()[0]\n",
    "                         for pred_label\n",
    "                         in pred_labels]\n",
    "\n",
    "            result = [x==y for (x,y) in zip(true,predicted)]\n",
    "            acc = sum(result)/len(result)\n",
    "            print(f\"Accuracy: {acc*100}%\")\n",
    "            accuracy_hist.append(acc)\n",
    "    print(\"Exiting validation...\")\n",
    "    net.train()\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct,total = 0,0\n",
    "ds_test =  AudioDataset(ds_name=\"test\", input_transform=transformations)\n",
    "test_loader = DataLoader(ds_test, batch_size=64, shuffle=True)\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the network on {len(test_loader)} audio examples: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e12c6458f51c06d979c888543cf47020d8394af5117571a8a2c66e9e9c5627f9"
  },
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
