{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "import pickle \n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch                                      # root package\n",
    "from torch.utils.data import Dataset, DataLoader  # data \n",
    "import torch.nn as nn                             # neural networks\n",
    "import torch.nn.functional as F                   # layers, activations and more\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio                                 # audio processing\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "def get_dataset(name):\n",
    "    df = pd.DataFrame.from_dict(pickle.load(open(f\"./data/{name}.p\", \"rb\")).items())\n",
    "    df.rename(columns={0:\"path\", 1:\"class\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"*\"*10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"*\"*10)\n",
    "    print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "    print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "    print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "    print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Amplitude\")\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "        \n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "    num_channels, _ = waveform.shape\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate, sides=\"onesided\")\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Frequency [Hz]\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_MP3_PATH = (\"/home/akinwilson/Projects/bird-sound-classifier/data/birds/Phylloscopuscollybita/Poland\"\n",
    "\"/Phylloscopuscollybita325319.mp3\")\n",
    "metadata = torchaudio.info(SAMPLE_MP3_PATH)\n",
    "print(\"metadata: \", metadata)\n",
    "def _get_sample(path, resample=None):\n",
    "    effects = [[\"remix\", \"1\"]]\n",
    "    if resample:\n",
    "        effects.extend(\n",
    "            [\n",
    "                [\"lowpass\", f\"{resample // 2}\"],\n",
    "                [\"rate\", f\"{resample}\"],\n",
    "            ]\n",
    "        )\n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_sample(SAMPLE_WAV_PATH, resample=None):\n",
    "    return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "SAMPLE_RATE = 32000\n",
    "waveform, sample_rate = get_sample(SAMPLE_MP3_PATH, resample=SAMPLE_RATE)\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "plot_specgram(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spectrogram = torchaudio.transforms.Spectrogram(normalized=True)\n",
    "transformations = torch.nn.Sequential(extract_spectrogram)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,ds_name, input_transform=None):\n",
    "        self.files = get_dataset(f\"{ds_name}\") # .sample(n=30)  # .samepl[:20]\n",
    "        self.cls2id = {name:idx for idx,name in enumerate(list(self.files['class'].unique()))}\n",
    "        self.input_transform = input_transform\n",
    "        self.NEW_SAMPLE_RATE = 16000\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.files.iloc[idx]['path']\n",
    "        data_label = self.files.iloc[idx]['class']\n",
    "        audio_data, OLD_SAMPLE_RATE = torchaudio.load(data_path, normalize=True)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:OLD_SAMPLE_RATE * 10], 0)\n",
    "        apply_resampling  = torchaudio.transforms.Resample(orig_freq= OLD_SAMPLE_RATE ,\n",
    "                                                        new_freq= self.NEW_SAMPLE_RATE,\n",
    "                                                        resampling_method= 'sinc_interpolation',\n",
    "                                                        lowpass_filter_width = 6,\n",
    "                                                        rolloff = 0.99)\n",
    "        audio_data = apply_resampling(audio_data)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE*10], 0)\n",
    "\n",
    "        while audio_data.shape[-1] < self.NEW_SAMPLE_RATE*10:\n",
    "            audio_data = torch.concat((audio_data,audio_data), dim=1)\n",
    "            audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE * 10], 0)\n",
    "\n",
    "            \n",
    "        if self.input_transform:\n",
    "            audio_input = self.input_transform(audio_data)\n",
    "\n",
    "\n",
    "        target_label = torch.tensor(self.cls2id[data_label], dtype=torch.int16).type(torch.LongTensor)        \n",
    "        return audio_input, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=6, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=6, out_channels=1, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.do = torch.nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(480, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.bn(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.bn(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.do(x)\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used for training: {device}\")\n",
    "\n",
    "\n",
    "net = Net(num_classes=19)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AudioDataset(ds_name=\"train\", input_transform=transformations)\n",
    "train_dataloader = DataLoader( ds_train,num_workers=4,batch_size=128, shuffle=True)\n",
    "loss_hist = []\n",
    "for epoch in tqdm(range(10)):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(\"loss: \", loss.item())\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss /len(train_dataloader)}')\n",
    "    loss_hist.append(loss.item())\n",
    "    running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "ds_test =  AudioDataset(ds_name=\"test\", input_transform=transformations)\n",
    "test_loader = DataLoader(ds_test, batch_size=10, shuffle=True)\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on {len(test_loader)} audio examples: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
