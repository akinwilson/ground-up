{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libtorch_hip.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000000?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m                   \u001b[39m# layers, activations and more\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000000?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m                                 \u001b[39m# audio processing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000000?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=2'>3</a>\u001b[0m     compliance,\n\u001b[1;32m      <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=3'>4</a>\u001b[0m     datasets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=10'>11</a>\u001b[0m     transforms,\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=14'>15</a>\u001b[0m     list_audio_backends,\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=15'>16</a>\u001b[0m     get_audio_backend,\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=16'>17</a>\u001b[0m     set_audio_backend,\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/__init__.py?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=22'>23</a>\u001b[0m     \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=23'>24</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchaudio  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=26'>27</a>\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py:21\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=15'>16</a>\u001b[0m \u001b[39m# In case `torchaudio` is deployed with `pex` format, this file does not exist.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=16'>17</a>\u001b[0m \u001b[39m# In this case, we expect that `libtorchaudio` is available somewhere\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=17'>18</a>\u001b[0m \u001b[39m# in the search path of dynamic loading mechanism, and importing `_torchaudio`,\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=18'>19</a>\u001b[0m \u001b[39m# which depends on `libtorchaudio` and dynamic loader will handle it for us.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m---> <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=20'>21</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=21'>22</a>\u001b[0m     torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mload_library(path)\n\u001b[1;32m     <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torchaudio/_extension.py?line=22'>23</a>\u001b[0m \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py:110\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=104'>105</a>\u001b[0m path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=105'>106</a>\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=106'>107</a>\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=107'>108</a>\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=108'>109</a>\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=109'>110</a>\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[1;32m    <a href='file:///home/akinwilson/.local/share/virtualenvs/ground-up-Awl4p5GG/lib/python3.8/site-packages/torch/_ops.py?line=110'>111</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/akinwilson/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py?line=369'>370</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    <a href='file:///home/akinwilson/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py?line=371'>372</a>\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/akinwilson/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py?line=372'>373</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    <a href='file:///home/akinwilson/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py?line=373'>374</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/akinwilson/.pyenv/versions/3.8.2/lib/python3.8/ctypes/__init__.py?line=374'>375</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libtorch_hip.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings \n",
    "import pickle \n",
    "\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch                                      # root package\n",
    "from torch.utils.data import Dataset, DataLoader  # data \n",
    "import torch.nn as nn                             # neural networks\n",
    "import torch.nn.functional as F                   # layers, activations and more\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio                                 # audio processing\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "def get_dataset(name):\n",
    "    df = pd.DataFrame.from_dict(pickle.load(open(f\"./data/{name}.p\", \"rb\")).items())\n",
    "    df.rename(columns={0:\"path\", 1:\"class\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"*\"*10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"*\"*10)\n",
    "    print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "    print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "    print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "    print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Amplitude\")\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "        \n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "    num_channels, _ = waveform.shape\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate, sides=\"onesided\")\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        axes[c].set_xlabel(\"Time [s]\")\n",
    "        axes[c].set_ylabel(\"Frequency [Hz]\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchaudio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000004?line=0'>1</a>\u001b[0m SAMPLE_MP3_PATH \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m/home/akinwilson/Projects/bird-sound-classifier/data/birds/Phylloscopuscollybita/Poland\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000004?line=1'>2</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39m/Phylloscopuscollybita325319.mp3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000004?line=2'>3</a>\u001b[0m metadata \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39minfo(SAMPLE_MP3_PATH)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmetadata: \u001b[39m\u001b[39m\"\u001b[39m, metadata)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_sample\u001b[39m(path, resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchaudio' is not defined"
     ]
    }
   ],
   "source": [
    "SAMPLE_MP3_PATH = (\"/home/akinwilson/Projects/bird-sound-classifier/data/birds/Phylloscopuscollybita/Poland\"\n",
    "\"/Phylloscopuscollybita325319.mp3\")\n",
    "metadata = torchaudio.info(SAMPLE_MP3_PATH)\n",
    "print(\"metadata: \", metadata)\n",
    "def _get_sample(path, resample=None):\n",
    "    effects = [[\"remix\", \"1\"]]\n",
    "    if resample:\n",
    "        effects.extend(\n",
    "            [\n",
    "                [\"lowpass\", f\"{resample // 2}\"],\n",
    "                [\"rate\", f\"{resample}\"],\n",
    "            ]\n",
    "        )\n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_sample(SAMPLE_WAV_PATH, resample=None):\n",
    "    return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "SAMPLE_RATE = 32000\n",
    "waveform, sample_rate = get_sample(SAMPLE_MP3_PATH, resample=SAMPLE_RATE)\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "plot_specgram(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchaudio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000006?line=0'>1</a>\u001b[0m extract_spectrogram \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mSpectrogram(normalized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000006?line=1'>2</a>\u001b[0m transformations \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential(extract_spectrogram)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Projects/ground-up/7_bird_sound_classification.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAudioDataset\u001b[39;00m(Dataset):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchaudio' is not defined"
     ]
    }
   ],
   "source": [
    "extract_spectrogram = torchaudio.transforms.Spectrogram(normalized=True)\n",
    "transformations = torch.nn.Sequential(extract_spectrogram)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,ds_name, input_transform=None):\n",
    "        \n",
    "        self.files = get_dataset(f\"{ds_name}\") # .sample(n=30)  # .samepl[:20]\n",
    "        self.cls2id = {name:idx for idx,name in enumerate(list(self.files['class'].unique()))}\n",
    "        self.input_transform = input_transform\n",
    "        self.NEW_SAMPLE_RATE = 16000\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.files.iloc[idx]['path']\n",
    "        data_label = self.files.iloc[idx]['class']\n",
    "        audio_data, OLD_SAMPLE_RATE = torchaudio.load(data_path, normalize=True)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:OLD_SAMPLE_RATE * 10], 0)\n",
    "        apply_resampling  = torchaudio.transforms.Resample(orig_freq= OLD_SAMPLE_RATE ,\n",
    "                                                        new_freq= self.NEW_SAMPLE_RATE,\n",
    "                                                        resampling_method= 'sinc_interpolation',\n",
    "                                                        lowpass_filter_width = 6,\n",
    "                                                        rolloff = 0.99)\n",
    "        audio_data = apply_resampling(audio_data)\n",
    "        audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE*10], 0)\n",
    "\n",
    "        while audio_data.shape[-1] < self.NEW_SAMPLE_RATE*10:\n",
    "            audio_data = torch.concat((audio_data,audio_data), dim=1)\n",
    "            audio_data = torch.unsqueeze(audio_data[0,:self.NEW_SAMPLE_RATE * 10], 0)\n",
    "\n",
    "            \n",
    "        if self.input_transform:\n",
    "            audio_input = self.input_transform(audio_data)\n",
    "\n",
    "        target_label = torch.tensor(self.cls2id[data_label], dtype=torch.int16).type(torch.LongTensor)        \n",
    "        return audio_input, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=6, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=6, out_channels=1, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.do = torch.nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(480, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.bn(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.bn(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.do(x)\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used for training: {device}\")\n",
    "\n",
    "\n",
    "net = Net(num_classes=19)\n",
    "net.eval()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AudioDataset(ds_name=\"train\", input_transform=transformations)\n",
    "train_dataloader = DataLoader( ds_train,num_workers=4,batch_size=128, shuffle=True)\n",
    "loss_hist = []\n",
    "for epoch in tqdm(range(10)):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(\"loss: \", loss.item())\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss /len(train_dataloader)}')\n",
    "    loss_hist.append(loss.item())\n",
    "    running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct,total = 0,0\n",
    "\n",
    "ds_test =  AudioDataset(ds_name=\"test\", input_transform=transformations)\n",
    "test_loader = DataLoader(ds_test, batch_size=10, shuffle=True)\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on {len(test_loader)} audio examples: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
