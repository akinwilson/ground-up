{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sy\n",
    "import scipy as sp\n",
    "from scipy import optimize as opt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "warnings.filterwarnings('ignore')\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cann a linear model learn the AND and XOR functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 0 & 0\\\\\n",
    "\\end{bmatrix},\n",
    "\\mathbf{y}_{and} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 0 & 0\\\\\n",
    "\\end{bmatrix},\n",
    "\\mathbf{y}_{xor} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\ \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use mean squared error for now, but notice binary ouput should be measured with binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sy.Matrix([[1,1,1],\n",
    "               [1,1,0],\n",
    "               [1,0,1],\n",
    "               [1,0,0]])\n",
    "y_xor = sy.Matrix([[0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]])\n",
    "y_and = sy.Matrix([[1],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [0]])\n",
    "w1,w2,w3 = sy.symbols(\"w_1,w_2,w_3\")\n",
    "w_xor = sy.Matrix([[w1],[w2],[w3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumed model is then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathbf{X}\\mathbf{w} = \\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 0 & 0\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\ \\end{bmatrix} =\\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining cost function for linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Gate: AND **********\n",
      "True:\n",
      " Matrix([[1], [0], [0], [0]])\n",
      "Pred:\n",
      " Matrix([[0.499999998947900], [0.249999995373210], [0.249999995373210], [-8.20147927349524e-9]])\n",
      "\n",
      "\n",
      "********** Gate: XOR **********\n",
      "True:\n",
      " Matrix([[0], [1], [1], [0]])\n",
      "Pred:\n",
      " Matrix([[0.833333337170653], [0.500000001636226], [0.500000001636226], [0.166666666101800]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def J(w,X, Y):\n",
    "    '''Cost function for linear model with mse'''\n",
    "    m = (1/4 * (X@w -Y).T @ (X@w - Y)).mean(0) \n",
    "    return sum(m) /len(m)\n",
    "\n",
    "\n",
    "def opts(J,w0, X,Y):\n",
    "    return opt.minimize(J, w0, args=(X,Y))\n",
    "\n",
    "\n",
    "def predictor(x,w_s):\n",
    "    return X@w_s\n",
    "\n",
    "def results(y, fixed_init=True):\n",
    "    w0_r = np.random.uniform(low=0.0, high=1.0, size=(3,1))\n",
    "    w0_f = np.array([[1/2],\n",
    "                     [1/2],\n",
    "                     [1/2]])\n",
    "    w0 = w0_f if fixed_init else w0_r\n",
    "    res = opts(J,\n",
    "             w0,\n",
    "             np.array(X).astype(np.float),\n",
    "             np.array(y).astype(np.float))\n",
    "    y_hat = predictor(X,res.x)\n",
    "    print(\"True:\\n\", y)\n",
    "    print(\"Pred:\\n\", sy.Matrix(y_hat))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for gate,y in zip([\"AND\",\"XOR\"],[y_and, y_xor]):\n",
    "    print(\"*\"*10,\"Gate:\", gate,\"*\"*10)\n",
    "    results(y)\n",
    "# Grad(J(X,w_xor,y_xor), w_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice!\n",
    "no matter how you initialise the optimization process, the XOR function will always have a degeneracy (equal output values for different inputs) and hence it can not be modelled with a shallow model.  \n",
    "\n",
    "Think about it this way: Look the the values of the pred matrix. Where can you cut the axis on which this values sit, in order to group and separate the outputs to either side of the cut? Impossible for the XOR gate with this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
