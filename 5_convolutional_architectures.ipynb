{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                      # root package\n",
    "from torch.utils.data import Dataset, DataLoader  # data \n",
    "import torch.autograd as autograd                 # computation graph\n",
    "from torch import Tensor                          # tensor node in the computation graph\n",
    "import torch.nn as nn                             # neural networks\n",
    "import torch.nn.functional as F                   # layers, activations and more\n",
    "import torch.optim as optim  \n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "- 5 multi-channel convolutions followed by 3 fully connected layers ~ 60 million parameters\n",
    "\n",
    "- First network (around 2012) to use a GPU for training to overcome network depth restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet\n",
    " - Designed to be small and low latency.\n",
    "  \n",
    " - Trainable on mobile devices.\n",
    " - Standard multi-channel convolutions expensive opertation \n",
    " - Where more than 1 filter used (with multi-channel convolutions), favourable to replace with **depth-wise** followed by **point-wise** convolutions. (layers together are called **depth-wise separable** filters)\n",
    " - MobileNet use 3 depth-wise separable layers\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet\n",
    "- Demonstrated two 3x3 filters are equvialent to a 5x5 receptive field\n",
    "\n",
    "- Most CNN networks will use a 3x3 kernel because of this reason. Less parameters and less likely to overfit.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception\n",
    "- Most CNNs stack their layers, Inception  concatenates in parallel filter outputs\n",
    "\n",
    "- Uses range of kernel sizes to allow learning multiple scales.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "- Uses *skip connections* \n",
    "\n",
    "- Surpassed human accuracy on Imagenet dataset\n",
    "  \n",
    "- Top 5 errror percentage; precentage of images where correct classification did not make it to the top 5 classes predicted by the network, for humans ~ 5%, for ResNet 3.6%.  First network to beat humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ground-up-Awl4p5GG",
   "language": "python",
   "name": "ground-up-awl4p5gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
